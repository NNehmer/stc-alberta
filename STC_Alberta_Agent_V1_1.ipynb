{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO4iCZSZgv0nbmFKvNZ2KT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NNehmer/stc-alberta/blob/main/STC_Alberta_Agent_V1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install --upgrade torch"
      ],
      "metadata": {
        "id": "nBR29Jz70jWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, sys\n",
        "print(\"Torch:\", torch.__version__, \"Python:\", sys.version.split()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_j3V3MK6U7O",
        "outputId": "a10541de-a9a5-4f75-c965-2e0dc692d9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 Python: 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) aktuelle Torch-Version:\n",
        "# %pip -q install --upgrade torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "\n",
        "# ---------------------------\n",
        "# Reproduzierbarkeit (optional)\n",
        "# ---------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# ==========================================\n",
        "# 1) STC-Bausteine\n",
        "# ==========================================\n",
        "class SpectralProjector(nn.Module):\n",
        "    \"\"\"Intentionaler Subraum-Projektor Π_S (QR-orthonormalisiert).\"\"\"\n",
        "    def __init__(self, latent_dim: int, intent_rank: int):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "        self.basis = nn.Parameter(torch.randn(latent_dim, intent_rank) * 0.1)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        Q, _ = torch.linalg.qr(self.basis)    # [D, r], orthonormal\n",
        "        Pi_S = Q @ Q.T                        # [D, D]\n",
        "        z_S  = z @ Pi_S\n",
        "        return z_S, Pi_S\n",
        "\n",
        "def coherence(z: torch.Tensor, z_S: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"κ(ψ) = ||Π_S ψ||² / ||ψ||²  (mit keepdim für sauberen Broadcast).\"\"\"\n",
        "    nz  = torch.norm(z,   dim=-1, keepdim=True) + 1e-8\n",
        "    nzS = torch.norm(z_S, dim=-1, keepdim=True)\n",
        "    return (nzS / nz) ** 2\n",
        "\n",
        "class ValueOperator(nn.Module):\n",
        "    \"\"\"Symmetrischer Wertoperator V; v(ψ)=⟨ψ|V|ψ⟩.\"\"\"\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(latent_dim, latent_dim) * 0.01)\n",
        "\n",
        "    def forward(self, z_S: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        V = 0.5 * (self.W + self.W.T)\n",
        "        v = torch.einsum('bi,ij,bj->b', z_S, V, z_S)\n",
        "        return v.unsqueeze(-1), V\n",
        "\n",
        "# ==========================================\n",
        "# 2) Agent\n",
        "# ==========================================\n",
        "class STCAlbertaAgent(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int, latent_dim: int = 64, intent_rank: int = 16):\n",
        "        super().__init__()\n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128), nn.LayerNorm(128), nn.ReLU(),\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.projector = SpectralProjector(latent_dim, intent_rank)\n",
        "        self.value_op  = ValueOperator(latent_dim)\n",
        "        # EMA-Target für TD-Stabilität\n",
        "        self.value_target = ValueOperator(latent_dim)\n",
        "        self.value_target.load_state_dict(self.value_op.state_dict())\n",
        "        self.value_tau = 0.005\n",
        "\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128), nn.ReLU(), nn.Linear(128, action_dim)\n",
        "        )\n",
        "        self.transition = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 128), nn.ReLU(), nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.reward_head = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_value_target(self):\n",
        "        for p, pt in zip(self.value_op.parameters(), self.value_target.parameters()):\n",
        "            pt.data.mul_(1 - self.value_tau).add_(self.value_tau * p.data)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        z = self.encoder(obs)                  # [B, D]\n",
        "        z_S, P = self.projector(z)             # [B, D], [D, D]\n",
        "        kappa = coherence(z, z_S)              # [B,1]\n",
        "        value, V = self.value_op(z_S)          # [B,1], [D,D]\n",
        "        logits = self.policy(z_S)              # [B,A]\n",
        "        return {\n",
        "            'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "            'value_matrix': V, 'kappa': kappa, 'value': value, 'logits': logits\n",
        "        }\n",
        "\n",
        "    def imagine(self, z_S: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"actions: [B] (indices) oder [B,A] (one-hot).\"\"\"\n",
        "        if actions.dim() == 1:\n",
        "            a_oh = F.one_hot(actions.long(), num_classes=self.action_dim).float()\n",
        "        elif actions.dim() == 2 and actions.size(-1) == self.action_dim:\n",
        "            a_oh = actions.float()\n",
        "        else:\n",
        "            raise ValueError(\"actions must be [B] (indices) or [B,A] (one-hot)\")\n",
        "        za = torch.cat([z_S, a_oh], dim=-1)\n",
        "        z_S_next = self.transition(za)\n",
        "        reward   = self.reward_head(za)\n",
        "        return z_S_next, reward\n",
        "\n",
        "    def select_action(self, obs: torch.Tensor, epsilon: float = 0.0) -> int:\n",
        "        with torch.no_grad():\n",
        "            out = self.forward(obs.unsqueeze(0))\n",
        "            if random.random() < epsilon:\n",
        "                return random.randint(0, self.action_dim - 1)\n",
        "            return out['logits'].argmax(dim=-1).item()\n",
        "\n",
        "# ==========================================\n",
        "# 3) Verlustfunktion (parametrisierbar)\n",
        "# ==========================================\n",
        "def stc_loss(\n",
        "    agent: STCAlbertaAgent,\n",
        "    obs: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor,\n",
        "    next_obs: torch.Tensor, dones: torch.Tensor,\n",
        "    gamma: float = 0.99,\n",
        "    lambda_kappa: float = 0.1,\n",
        "    lambda_comm: float = 0.01,\n",
        "    lambda_cons: float = 1.0,\n",
        "    entropy_coef: float = 5e-4,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "\n",
        "    out      = agent(obs)\n",
        "    out_next = agent(next_obs)\n",
        "\n",
        "    z_S, P, V   = out['latent_S'], out['projector'], out['value_matrix']\n",
        "    kappa       = out['kappa']\n",
        "    value, logits = out['value'], out['logits']\n",
        "\n",
        "    dist  = torch.distributions.Categorical(logits=logits)\n",
        "    logp  = dist.log_prob(actions)\n",
        "\n",
        "    # TD-Target mit EMA-Target\n",
        "    with torch.no_grad():\n",
        "        v_next_target, _ = agent.value_target(out_next['latent_S'])\n",
        "        td_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "\n",
        "    td_err      = td_target - value\n",
        "    policy_loss = -(logp * td_err.squeeze(-1).detach()).mean()\n",
        "    value_loss  = F.mse_loss(value, td_target)\n",
        "    entropy     = dist.entropy().mean()\n",
        "    L_RL        = policy_loss + value_loss - entropy_coef * entropy\n",
        "\n",
        "    # Relevanz-Maximierung\n",
        "    L_rel  = -kappa.mean()\n",
        "\n",
        "    # Kommutator-Penalty (Frobenius, skaliert)\n",
        "    comm   = V @ P - P @ V\n",
        "    L_comm = (torch.norm(comm, p='fro') ** 2) / V.numel()\n",
        "\n",
        "    # Konsistenz: Modell-Q vs Target-Q\n",
        "    z_S_next_pred, reward_pred = agent.imagine(z_S, actions)\n",
        "    with torch.no_grad():\n",
        "        v_next_pred_target, _ = agent.value_target(z_S_next_pred)\n",
        "    q_pred   = reward_pred + gamma * v_next_pred_target * (1 - dones.unsqueeze(-1))\n",
        "    q_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "    L_cons   = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "    total = L_RL + lambda_kappa * L_rel + lambda_comm * L_comm + lambda_cons * L_cons\n",
        "    return {\n",
        "        'total_loss': total, 'L_RL': L_RL, 'L_rel': L_rel, 'L_comm': L_comm, 'L_cons': L_cons,\n",
        "        'kappa': kappa.mean(), 'td_error': td_err.abs().mean(),\n",
        "        'policy_loss': policy_loss, 'value_loss': value_loss, 'entropy': entropy\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# 4) Einfache 2D-Nav-Umwelt (Reward-Shaping optional)\n",
        "# ==========================================\n",
        "class SimpleControlEnv:\n",
        "    \"\"\"\n",
        "    State: [x, y, goal_x, goal_y, vx, vy] (6D)\n",
        "    Actions: 0:up, 1:down, 2:left, 3:right\n",
        "    Reward:  -distance + progress_bonus * (prev_dist - dist)\n",
        "    Done:    dist < 0.1 oder steps >= max_steps\n",
        "    \"\"\"\n",
        "    def __init__(self, max_steps: int = 200, progress_bonus: float = 0.0):\n",
        "        self.state_dim  = 6\n",
        "        self.action_dim = 4\n",
        "        self.max_steps  = max_steps\n",
        "        self.progress_bonus = progress_bonus\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        self.pos   = np.random.uniform(-1, 1, 2)\n",
        "        self.vel   = np.zeros(2)\n",
        "        self.goal  = np.random.uniform(-1, 1, 2)\n",
        "        self.steps = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        return np.concatenate([self.pos, self.goal, self.vel])\n",
        "\n",
        "    def step(self, action: int):\n",
        "        prev_pos  = self.pos.copy()\n",
        "        prev_dist = float(np.linalg.norm(prev_pos - self.goal))\n",
        "\n",
        "        acc_map = {0: np.array([0, 0.1]), 1: np.array([0, -0.1]),\n",
        "                   2: np.array([-0.1, 0]), 3: np.array([0.1, 0])}\n",
        "        acc  = acc_map[action]\n",
        "        self.vel = np.clip(0.9 * self.vel + acc, -0.5, 0.5)\n",
        "        self.pos = np.clip(self.pos + self.vel, -1,  1)\n",
        "\n",
        "        dist   = float(np.linalg.norm(self.pos - self.goal))\n",
        "        reward = -dist + self.progress_bonus * (prev_dist - dist)\n",
        "\n",
        "        self.steps += 1\n",
        "        done = (dist < 0.1) or (self.steps >= self.max_steps)\n",
        "        return self._get_obs(), reward, done, {'distance': dist}\n",
        "\n",
        "# ==========================================\n",
        "# 5) Replay Buffer\n",
        "# ==========================================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    def push(self, obs, action, reward, next_obs, done):\n",
        "        self.buffer.append((obs, action, reward, next_obs, done))\n",
        "    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
        "        return {\n",
        "            'obs':      torch.FloatTensor(np.array(obs)),\n",
        "            'actions':  torch.LongTensor(actions),\n",
        "            'rewards':  torch.FloatTensor(rewards),\n",
        "            'next_obs': torch.FloatTensor(np.array(next_obs)),\n",
        "            'dones':    torch.FloatTensor(dones)\n",
        "        }\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ==========================================\n",
        "# 6) Training (parametrisierbar)\n",
        "# ==========================================\n",
        "def train_stc_alberta(\n",
        "    num_episodes: int = 80,\n",
        "    batch_size: int = 64,\n",
        "    buffer_size: int = 10000,\n",
        "    learning_rate: float = 3e-4,\n",
        "    epsilon_start: float = 0.5,\n",
        "    epsilon_end: float = 0.05,\n",
        "    epsilon_decay: float = 0.992,\n",
        "    print_every: int = 10,\n",
        "    # neue Parameter:\n",
        "    lambda_kappa: float = 0.1,\n",
        "    lambda_comm: float = 0.01,\n",
        "    lambda_cons: float = 1.0,\n",
        "    gamma: float = 0.99,\n",
        "    entropy_coef: float = 5e-4,\n",
        "    progress_bonus: float = 0.0,\n",
        "):\n",
        "    env      = SimpleControlEnv(progress_bonus=progress_bonus)\n",
        "    agent    = STCAlbertaAgent(obs_dim=env.state_dim, action_dim=env.action_dim, latent_dim=64, intent_rank=16)\n",
        "    optimizer= torch.optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "    buffer   = ReplayBuffer(capacity=buffer_size)\n",
        "\n",
        "    epsilon  = epsilon_start\n",
        "    ep_rewards, ep_kappas, ep_dists = [], [], []\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"STC–Alberta Agent Training\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Environment: SimpleControlEnv (2D)\")\n",
        "    print(f\"State dim: {env.state_dim}, Action dim: {env.action_dim}\")\n",
        "    print(\"Agent latent dim: 64, Intent rank: 16\")\n",
        "    print(\"=\"*70); print()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        ep_k_list: List[float] = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon)\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = agent(torch.FloatTensor(obs).unsqueeze(0))\n",
        "                ep_k_list.append(out['kappa'].item())\n",
        "\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "        ep_kappas.append(float(np.mean(ep_k_list)))\n",
        "        ep_dists.append(info['distance'])\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = buffer.sample(batch_size)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(\n",
        "                agent, **batch,\n",
        "                gamma=gamma,\n",
        "                lambda_kappa=lambda_kappa,\n",
        "                lambda_comm=lambda_comm,\n",
        "                lambda_cons=lambda_cons,\n",
        "                entropy_coef=entropy_coef,\n",
        "            )\n",
        "            losses['total_loss'].backward()\n",
        "            torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "        if (episode + 1) % print_every == 0:\n",
        "            avgR = float(np.mean(ep_rewards[-print_every:]))\n",
        "            avgK = float(np.mean(ep_kappas[-print_every:]))\n",
        "            avgD = float(np.mean(ep_dists[-print_every:]))\n",
        "            msg  = (f\"Episode {episode+1:4d} | Reward: {avgR:7.2f} | κ: {avgK:.3f} \"\n",
        "                    f\"| Dist: {avgD:.3f} | ε: {epsilon:.3f}\")\n",
        "            if len(buffer) >= batch_size:\n",
        "                msg += (f\"\\n             | L_RL: {losses['L_RL'].item():.4f} \"\n",
        "                        f\"| L_rel: {losses['L_rel'].item():.4f} \"\n",
        "                        f\"| L_comm: {losses['L_comm'].item():.6f} \"\n",
        "                        f\"| L_cons: {losses['L_cons'].item():.4f} \"\n",
        "                        f\"| H: {losses['entropy'].item():.3f}\")\n",
        "            print(msg)\n",
        "\n",
        "    # Korrelation (letzte 30)\n",
        "    k_tail = np.array(ep_kappas[-30:], dtype=np.float32)\n",
        "    r_tail = np.array(ep_rewards[-30:], dtype=np.float32)\n",
        "    corr = float(np.corrcoef(k_tail, r_tail)[0, 1]) if len(k_tail) > 1 else float(\"nan\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*70)\n",
        "    print(\"Training Complete\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Final κ (last 30): {np.mean(k_tail):.3f}\")\n",
        "    print(f\"Final reward (last 30): {np.mean(r_tail):.2f}\")\n",
        "    print(f\"Final distance (last 30): {np.mean(ep_dists[-30:]):.3f}\")\n",
        "    print(f\"Corr(Return, κ) (last 30): {corr:.3f}\")\n",
        "\n",
        "    return agent, ep_rewards, ep_kappas, ep_dists\n",
        "\n",
        "# ==========================================\n",
        "# 7) Ablationen\n",
        "# ==========================================\n",
        "def run_ablation_no_projector(num_episodes: int = 80) -> float:\n",
        "    \"\"\"A1: Kein Π_S (volle latente Fläche).\"\"\"\n",
        "    print(\"\\n\"+\"=\"*70); print(\"ABLATION A1: No Projector (Full Latent Space)\"); print(\"=\"*70)\n",
        "\n",
        "    class NoProjAgent(STCAlbertaAgent):\n",
        "        def forward(self, obs):\n",
        "            z   = self.encoder(obs)\n",
        "            z_S = z\n",
        "            P   = torch.eye(self.latent_dim, device=z.device)\n",
        "            k   = torch.ones(z.shape[0], 1, device=z.device)\n",
        "            v, V = self.value_op(z_S)\n",
        "            logits = self.policy(z_S)\n",
        "            return {'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "                    'value_matrix': V, 'kappa': k, 'value': v, 'logits': logits}\n",
        "\n",
        "    env = SimpleControlEnv()\n",
        "    agent = NoProjAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=3e-4)\n",
        "    buffer = ReplayBuffer()\n",
        "    ep_rewards: List[float] = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon=0.1)\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= 64:\n",
        "            batch = buffer.sample(64)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch, lambda_kappa=0.0)  # keine κ-Reg.\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    val = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"Final reward (A1, last 30): {val:.2f}\")\n",
        "    return val\n",
        "\n",
        "def run_ablation_random_projector(num_episodes: int = 80) -> float:\n",
        "    \"\"\"A2: Zufälliger (fixierter) Π_S (ohne Lernen) — sollte schlechter sein als gelernter Π_S.\"\"\"\n",
        "    print(\"\\n\"+\"=\"*70); print(\"ABLATION A2: Random Fixed Projector\"); print(\"=\"*70)\n",
        "\n",
        "    class RandProjAgent(STCAlbertaAgent):\n",
        "        def __init__(self, obs_dim, action_dim, latent_dim=64, intent_rank=16):\n",
        "            super().__init__(obs_dim, action_dim, latent_dim, intent_rank)\n",
        "            with torch.no_grad():\n",
        "                Q, _ = torch.linalg.qr(torch.randn(latent_dim, intent_rank))\n",
        "                self.register_buffer('Q_fixed', Q)\n",
        "        def forward(self, obs):\n",
        "            z = self.encoder(obs)\n",
        "            Q = self.Q_fixed\n",
        "            P = Q @ Q.T\n",
        "            z_S = z @ P\n",
        "            k   = coherence(z, z_S)\n",
        "            v, V = self.value_op(z_S)\n",
        "            logits = self.policy(z_S)\n",
        "            return {'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "                    'value_matrix': V, 'kappa': k, 'value': v, 'logits': logits}\n",
        "\n",
        "    env = SimpleControlEnv()\n",
        "    agent = RandProjAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=3e-4)\n",
        "    buffer = ReplayBuffer()\n",
        "    ep_rewards: List[float] = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon=0.1)\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= 64:\n",
        "            batch = buffer.sample(64)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch)  # mit Standard-λ; Π_S ist aber nicht lernend\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    val = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"Final reward (A2, last 30): {val:.2f}\")\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "gbTmOfDm-j-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Schneller Trainingslauf (Colab-freundlich). Werte gern anpassen.\n",
        "agent, rewards, kappas, dists = train_stc_alberta(\n",
        "    num_episodes=80,\n",
        "    batch_size=64,\n",
        "    learning_rate=3e-4,\n",
        "    print_every=10,\n",
        "    # neue Hebel:\n",
        "    lambda_cons=1.0,\n",
        "    lambda_kappa=0.1,\n",
        "    lambda_comm=0.01,      # A3-Check: auf 0.0 setzen, um [V, Π_S] abzuschalten\n",
        "    gamma=0.99,\n",
        "    entropy_coef=5e-4,\n",
        "    progress_bonus=0.05,    # Reward-Shaping: 0.0 (aus) oder z.B. 0.05\n",
        ")\n",
        "\n",
        "# Ablationen (optional)\n",
        "a1 = run_ablation_no_projector(num_episodes=80)\n",
        "a2 = run_ablation_random_projector(num_episodes=80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"STC–Alberta (with Π_S): {np.mean(rewards[-30:]):.2f} (last 30 eps)\")\n",
        "print(f\"Ablation A1 (no Π_S):   {a1:.2f} (last 30 eps)\")\n",
        "print(f\"Ablation A2 (rand Π_S): {a2:.2f} (last 30 eps)\")\n",
        "print(f\"Δ vs A1:                {np.mean(rewards[-30:]) - a1:.2f}\")\n",
        "print(f\"Δ vs A2:                {np.mean(rewards[-30:]) - a2:.2f}\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j03GO49M-1RI",
        "outputId": "cac59102-c5a3-403d-e314-54b4f298606e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STC–Alberta Agent Training\n",
            "======================================================================\n",
            "Environment: SimpleControlEnv (2D)\n",
            "State dim: 6, Action dim: 4\n",
            "Agent latent dim: 64, Intent rank: 16\n",
            "======================================================================\n",
            "\n",
            "Episode   10 | Reward: -273.08 | κ: 0.311 | Dist: 1.134 | ε: 0.461\n",
            "             | L_RL: 0.1349 | L_rel: -0.4114 | L_comm: 0.000021 | L_cons: 2.3659 | H: 1.386\n",
            "Episode   20 | Reward: -247.62 | κ: 0.484 | Dist: 1.181 | ε: 0.426\n",
            "             | L_RL: 1.2288 | L_rel: -0.6121 | L_comm: 0.000025 | L_cons: 1.5176 | H: 1.382\n",
            "Episode   30 | Reward: -262.85 | κ: 0.578 | Dist: 1.143 | ε: 0.393\n",
            "             | L_RL: 0.0820 | L_rel: -0.6225 | L_comm: 0.000026 | L_cons: 0.8833 | H: 1.382\n",
            "Episode   40 | Reward: -285.02 | κ: 0.650 | Dist: 1.406 | ε: 0.363\n",
            "             | L_RL: 0.5004 | L_rel: -0.7068 | L_comm: 0.000026 | L_cons: 0.3785 | H: 1.382\n",
            "Episode   50 | Reward: -337.46 | κ: 0.744 | Dist: 1.726 | ε: 0.335\n",
            "             | L_RL: 0.1314 | L_rel: -0.7613 | L_comm: 0.000028 | L_cons: 0.3830 | H: 1.372\n",
            "Episode   60 | Reward: -292.02 | κ: 0.758 | Dist: 1.540 | ε: 0.309\n",
            "             | L_RL: 0.0734 | L_rel: -0.7675 | L_comm: 0.000029 | L_cons: 0.2374 | H: 1.358\n",
            "Episode   70 | Reward: -271.57 | κ: 0.757 | Dist: 1.345 | ε: 0.285\n",
            "             | L_RL: 0.0941 | L_rel: -0.7776 | L_comm: 0.000031 | L_cons: 0.2583 | H: 1.356\n",
            "Episode   80 | Reward: -213.26 | κ: 0.765 | Dist: 1.130 | ε: 0.263\n",
            "             | L_RL: 0.0299 | L_rel: -0.8007 | L_comm: 0.000032 | L_cons: 0.1698 | H: 1.330\n",
            "\n",
            "======================================================================\n",
            "Training Complete\n",
            "======================================================================\n",
            "Final κ (last 30): 0.760\n",
            "Final reward (last 30): -258.95\n",
            "Final distance (last 30): 1.338\n",
            "Corr(Return, κ) (last 30): -0.633\n",
            "\n",
            "======================================================================\n",
            "ABLATION A1: No Projector (Full Latent Space)\n",
            "======================================================================\n",
            "Final reward (A1, last 30): -173.67\n",
            "\n",
            "======================================================================\n",
            "ABLATION A2: Random Fixed Projector\n",
            "======================================================================\n",
            "Final reward (A2, last 30): -287.58\n",
            "\n",
            "======================================================================\n",
            "RESULTS SUMMARY\n",
            "======================================================================\n",
            "STC–Alberta (with Π_S): -258.95 (last 30 eps)\n",
            "Ablation A1 (no Π_S):   -173.67 (last 30 eps)\n",
            "Ablation A2 (rand Π_S): -287.58 (last 30 eps)\n",
            "Δ vs A1:                -85.28\n",
            "Δ vs A2:                28.63\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}