{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/MeYyzCknR46WhnxF/xSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NNehmer/stc-alberta/blob/main/STC_Alberta_Agent_V1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install --upgrade torch"
      ],
      "metadata": {
        "id": "nBR29Jz70jWn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxSsOROlzJzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, sys\n",
        "print(\"Torch:\", torch.__version__, \"Python:\", sys.version.split()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_j3V3MK6U7O",
        "outputId": "78accb2d-9b66-4fe7-bc8b-cd4d17bce04c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 Python: 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "STC-Alberta Agent V2.0 (Improved)\n",
        "\n",
        "Key improvements:\n",
        "- Cleaner dual-optimizer without grad freeze dance\n",
        "- Hyperparameter dataclass for clarity\n",
        "- Better logging with projector rank tracking\n",
        "- All three ablations (A1, A2, A3)\n",
        "- Automatic correlation analysis\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class STCConfig:\n",
        "    \"\"\"Hyperparameters in one place\"\"\"\n",
        "    # Architecture\n",
        "    obs_dim: int = 6\n",
        "    action_dim: int = 4\n",
        "    latent_dim: int = 64\n",
        "    intent_rank: int = 16\n",
        "\n",
        "    # Training\n",
        "    num_episodes: int = 200\n",
        "    batch_size: int = 64\n",
        "    buffer_size: int = 10000\n",
        "    lr_main: float = 3e-4\n",
        "    lr_proj: float = 1e-4\n",
        "    gamma: float = 0.99\n",
        "\n",
        "    # Exploration\n",
        "    epsilon_start: float = 0.5\n",
        "    epsilon_end: float = 0.05\n",
        "    epsilon_decay: float = 0.992\n",
        "\n",
        "    # Loss weights (final values)\n",
        "    lambda_kappa: float = 0.05\n",
        "    lambda_comm: float = 0.01\n",
        "    lambda_cons: float = 1.0\n",
        "    entropy_coef: float = 3e-4\n",
        "\n",
        "    # Scheduling\n",
        "    warmup_episodes: int = 30\n",
        "    ramp_episodes: int = 80\n",
        "\n",
        "    # Projector update\n",
        "    proj_update_every: int = 4\n",
        "    adv_weight_kappa: bool = True\n",
        "    adv_temp: float = 0.5\n",
        "\n",
        "    # Stability\n",
        "    value_tau: float = 0.005\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # Environment\n",
        "    max_steps: int = 200\n",
        "    progress_bonus: float = 1.0  # Enable reward shaping\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Spectral Components\n",
        "# ============================================================================\n",
        "\n",
        "class SpectralProjector(nn.Module):\n",
        "    def __init__(self, latent_dim: int, intent_rank: int):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "        self.basis = nn.Parameter(torch.randn(latent_dim, intent_rank) * 0.1)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        Q, _ = torch.linalg.qr(self.basis)\n",
        "        Pi_S = Q @ Q.T\n",
        "        z_S = z @ Pi_S\n",
        "        return z_S, Pi_S\n",
        "\n",
        "    def get_effective_rank(self) -> float:\n",
        "        \"\"\"Compute effective rank via singular values\"\"\"\n",
        "        with torch.no_grad():\n",
        "            Q, _ = torch.linalg.qr(self.basis)\n",
        "            Pi_S = Q @ Q.T\n",
        "            s = torch.linalg.svdvals(Pi_S)\n",
        "            s_norm = s / s.sum()\n",
        "            entropy = -(s_norm * torch.log(s_norm + 1e-10)).sum()\n",
        "            return torch.exp(entropy).item()\n",
        "\n",
        "\n",
        "def coherence(z: torch.Tensor, z_S: torch.Tensor) -> torch.Tensor:\n",
        "    norm_z = torch.norm(z, dim=-1, keepdim=True) + 1e-8\n",
        "    norm_z_S = torch.norm(z_S, dim=-1, keepdim=True)\n",
        "    return (norm_z_S / norm_z) ** 2\n",
        "\n",
        "\n",
        "class ValueOperator(nn.Module):\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(latent_dim, latent_dim) * 0.01)\n",
        "\n",
        "    def forward(self, z_S: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        V = 0.5 * (self.W + self.W.T)\n",
        "        v = torch.einsum('bi,ij,bj->b', z_S, V, z_S)\n",
        "        return v.unsqueeze(-1), V\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Agent\n",
        "# ============================================================================\n",
        "\n",
        "class STCAlbertaAgent(nn.Module):\n",
        "    def __init__(self, config: STCConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(config.obs_dim, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, config.latent_dim)\n",
        "        )\n",
        "\n",
        "        # Spectral core\n",
        "        self.projector = SpectralProjector(config.latent_dim, config.intent_rank)\n",
        "        self.value_op = ValueOperator(config.latent_dim)\n",
        "\n",
        "        # EMA target\n",
        "        self.value_target = ValueOperator(config.latent_dim)\n",
        "        self.value_target.load_state_dict(self.value_op.state_dict())\n",
        "        for p in self.value_target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Policy\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(config.latent_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, config.action_dim)\n",
        "        )\n",
        "\n",
        "        # World model\n",
        "        self.transition = nn.Sequential(\n",
        "            nn.Linear(config.latent_dim + config.action_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, config.latent_dim)\n",
        "        )\n",
        "\n",
        "        self.reward_head = nn.Sequential(\n",
        "            nn.Linear(config.latent_dim + config.action_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, use_projection: bool = True) -> Dict[str, torch.Tensor]:\n",
        "        z = self.encoder(obs)\n",
        "        z_S, Pi_S = self.projector(z)\n",
        "\n",
        "        # For policy: use z_S only if projection enabled\n",
        "        policy_input = z_S if use_projection else z\n",
        "\n",
        "        kappa = coherence(z, z_S)\n",
        "        value, V = self.value_op(z_S)\n",
        "        logits = self.policy(policy_input)\n",
        "\n",
        "        return {\n",
        "            'latent': z,\n",
        "            'latent_S': z_S,\n",
        "            'projector': Pi_S,\n",
        "            'value_matrix': V,\n",
        "            'kappa': kappa,\n",
        "            'value': value,\n",
        "            'logits': logits\n",
        "        }\n",
        "\n",
        "    def forward_proj_only(self, obs: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Forward pass computing only spectral quantities (for separate proj update)\"\"\"\n",
        "        with torch.no_grad():\n",
        "            z = self.encoder(obs)\n",
        "\n",
        "        z_S, Pi_S = self.projector(z)\n",
        "        kappa = coherence(z, z_S)\n",
        "        _, V = self.value_op(z_S)\n",
        "\n",
        "        return {\n",
        "            'latent': z,\n",
        "            'latent_S': z_S,\n",
        "            'projector': Pi_S,\n",
        "            'value_matrix': V,\n",
        "            'kappa': kappa\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_value_target(self):\n",
        "        tau = self.config.value_tau\n",
        "        for p, pt in zip(self.value_op.parameters(), self.value_target.parameters()):\n",
        "            pt.data.mul_(1 - tau).add_(tau * p.data)\n",
        "\n",
        "    def imagine(self, z_S: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if actions.dim() == 1:\n",
        "            a_oh = F.one_hot(actions.long(), self.config.action_dim).float()\n",
        "        else:\n",
        "            a_oh = actions.float()\n",
        "        za = torch.cat([z_S, a_oh], dim=-1)\n",
        "        z_S_next = self.transition(za)\n",
        "        reward = self.reward_head(za)\n",
        "        return z_S_next, reward\n",
        "\n",
        "    def select_action(self, obs: torch.Tensor, epsilon: float = 0.0, use_projection: bool = True) -> int:\n",
        "        with torch.no_grad():\n",
        "            out = self.forward(obs.unsqueeze(0), use_projection=use_projection)\n",
        "            if random.random() < epsilon:\n",
        "                return random.randint(0, self.config.action_dim - 1)\n",
        "            return out['logits'].argmax(dim=-1).item()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Loss Functions\n",
        "# ============================================================================\n",
        "\n",
        "def compute_main_loss(\n",
        "    agent: STCAlbertaAgent,\n",
        "    obs: torch.Tensor,\n",
        "    actions: torch.Tensor,\n",
        "    rewards: torch.Tensor,\n",
        "    next_obs: torch.Tensor,\n",
        "    dones: torch.Tensor,\n",
        "    lambda_cons: float,\n",
        "    entropy_coef: float,\n",
        "    use_projection: bool = True\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Main loss: RL + world model consistency\"\"\"\n",
        "\n",
        "    out = agent.forward(obs, use_projection=use_projection)\n",
        "    out_next = agent.forward(next_obs, use_projection=use_projection)\n",
        "\n",
        "    z_S = out['latent_S']\n",
        "    value = out['value']\n",
        "    logits = out['logits']\n",
        "\n",
        "    # Policy loss with advantage normalization\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "    log_prob = dist.log_prob(actions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        v_target, _ = agent.value_target(out_next['latent_S'])\n",
        "        td_target = rewards.unsqueeze(-1) + agent.config.gamma * v_target * (1 - dones.unsqueeze(-1))\n",
        "\n",
        "    td_error = td_target - value\n",
        "    adv = td_error.squeeze(-1).detach()\n",
        "    adv_norm = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "\n",
        "    policy_loss = -(log_prob * adv_norm).mean()\n",
        "    value_loss = F.mse_loss(value, td_target)\n",
        "    entropy = dist.entropy().mean()\n",
        "\n",
        "    L_RL = policy_loss + value_loss - entropy_coef * entropy\n",
        "\n",
        "    # World model consistency\n",
        "    z_S_next_pred, reward_pred = agent.imagine(z_S, actions)\n",
        "    with torch.no_grad():\n",
        "        v_next_pred, _ = agent.value_target(z_S_next_pred)\n",
        "\n",
        "    q_pred = reward_pred + agent.config.gamma * v_next_pred * (1 - dones.unsqueeze(-1))\n",
        "    q_target = rewards.unsqueeze(-1) + agent.config.gamma * v_target * (1 - dones.unsqueeze(-1))\n",
        "    L_cons = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "    total = L_RL + lambda_cons * L_cons\n",
        "\n",
        "    return {\n",
        "        'total_loss': total,\n",
        "        'L_RL': L_RL,\n",
        "        'L_cons': L_cons,\n",
        "        'policy_loss': policy_loss,\n",
        "        'value_loss': value_loss,\n",
        "        'entropy': entropy,\n",
        "        'td_error': td_error.abs().mean(),\n",
        "        'adv_mean': adv.mean(),\n",
        "        'adv_pos_frac': (adv > 0).float().mean()\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_proj_loss(\n",
        "    agent: STCAlbertaAgent,\n",
        "    obs: torch.Tensor,\n",
        "    actions: torch.Tensor,\n",
        "    rewards: torch.Tensor,\n",
        "    next_obs: torch.Tensor,\n",
        "    dones: torch.Tensor,\n",
        "    lambda_kappa: float,\n",
        "    lambda_comm: float,\n",
        "    adv_weight: bool = False,\n",
        "    adv_temp: float = 0.5\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Projector loss: coherence + commutator\"\"\"\n",
        "\n",
        "    out = agent.forward_proj_only(obs)\n",
        "\n",
        "    z_S = out['latent_S']\n",
        "    Pi_S = out['projector']\n",
        "    V = out['value_matrix']\n",
        "    kappa = out['kappa']\n",
        "\n",
        "    # Coherence loss (optionally advantage-weighted)\n",
        "    if adv_weight:\n",
        "        # Compute advantage from frozen main network\n",
        "        with torch.no_grad():\n",
        "            out_full = agent.forward(obs, use_projection=True)\n",
        "            out_next = agent.forward(next_obs, use_projection=True)\n",
        "            v_target, _ = agent.value_target(out_next['latent_S'])\n",
        "            td_target = rewards.unsqueeze(-1) + agent.config.gamma * v_target * (1 - dones.unsqueeze(-1))\n",
        "            adv = (td_target - out_full['value']).squeeze(-1)\n",
        "\n",
        "        w = torch.sigmoid(adv / adv_temp)\n",
        "        w = w / (w.mean() + 1e-8)\n",
        "        L_rel = -(w * kappa.squeeze(-1)).mean()\n",
        "    else:\n",
        "        L_rel = -kappa.mean()\n",
        "\n",
        "    # Commutator penalty\n",
        "    comm = V @ Pi_S - Pi_S @ V\n",
        "    L_comm = (torch.norm(comm, p='fro') ** 2) / V.numel()\n",
        "\n",
        "    total = lambda_kappa * L_rel + lambda_comm * L_comm\n",
        "\n",
        "    return {\n",
        "        'total_loss': total,\n",
        "        'L_rel': L_rel,\n",
        "        'L_comm': L_comm,\n",
        "        'kappa': kappa.mean()\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Environment\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleControlEnv:\n",
        "    def __init__(self, max_steps: int = 200, progress_bonus: float = 0.0):\n",
        "        self.state_dim = 6\n",
        "        self.action_dim = 4\n",
        "        self.max_steps = max_steps\n",
        "        self.progress_bonus = progress_bonus\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        self.pos = np.random.uniform(-1, 1, 2)\n",
        "        self.vel = np.zeros(2)\n",
        "        self.goal = np.random.uniform(-1, 1, 2)\n",
        "        self.steps = 0\n",
        "        self.prev_dist = float(np.linalg.norm(self.pos - self.goal))\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        return np.concatenate([self.pos, self.goal, self.vel])\n",
        "\n",
        "    def step(self, action: int):\n",
        "        acc_map = {0: np.array([0, 0.1]), 1: np.array([0, -0.1]),\n",
        "                   2: np.array([-0.1, 0]), 3: np.array([0.1, 0])}\n",
        "        acc = acc_map[action]\n",
        "\n",
        "        self.vel = np.clip(0.9 * self.vel + acc, -0.5, 0.5)\n",
        "        self.pos = np.clip(self.pos + self.vel, -1, 1)\n",
        "\n",
        "        dist = float(np.linalg.norm(self.pos - self.goal))\n",
        "        progress = self.prev_dist - dist\n",
        "        reward = -dist + self.progress_bonus * progress\n",
        "        self.prev_dist = dist\n",
        "\n",
        "        self.steps += 1\n",
        "        done = (dist < 0.1) or (self.steps >= self.max_steps)\n",
        "\n",
        "        return self._get_obs(), reward, done, {'distance': dist}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Replay Buffer\n",
        "# ============================================================================\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, obs, action, reward, next_obs, done):\n",
        "        self.buffer.append((obs, action, reward, next_obs, done))\n",
        "\n",
        "    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
        "        return {\n",
        "            'obs': torch.FloatTensor(np.array(obs)),\n",
        "            'actions': torch.LongTensor(actions),\n",
        "            'rewards': torch.FloatTensor(rewards),\n",
        "            'next_obs': torch.FloatTensor(np.array(next_obs)),\n",
        "            'dones': torch.FloatTensor(dones)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Training\n",
        "# ============================================================================\n",
        "\n",
        "def train_stc_alberta(config: STCConfig, print_every: int = 10):\n",
        "    env = SimpleControlEnv(max_steps=config.max_steps, progress_bonus=config.progress_bonus)\n",
        "    agent = STCAlbertaAgent(config)\n",
        "\n",
        "    # Separate optimizers\n",
        "    main_params = [p for n, p in agent.named_parameters()\n",
        "                   if not n.startswith('projector.')]\n",
        "    proj_params = list(agent.projector.parameters())\n",
        "\n",
        "    optimizer_main = torch.optim.Adam(main_params, lr=config.lr_main)\n",
        "    optimizer_proj = torch.optim.Adam(proj_params, lr=config.lr_proj)\n",
        "\n",
        "    buffer = ReplayBuffer(capacity=config.buffer_size)\n",
        "    epsilon = config.epsilon_start\n",
        "\n",
        "    ep_rewards, ep_kappas, ep_dists = [], [], []\n",
        "    comm_norms = []\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STC-Alberta Agent V2.0 Training\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Config: {config.num_episodes} episodes, rank {config.intent_rank}/{config.latent_dim}\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "    for episode in range(config.num_episodes):\n",
        "        # Ramp schedule\n",
        "        if episode < config.warmup_episodes:\n",
        "            ramp = 0.0\n",
        "        else:\n",
        "            ramp = min(1.0, (episode - config.warmup_episodes) / max(1, config.ramp_episodes))\n",
        "\n",
        "        lambda_kappa_eff = config.lambda_kappa * ramp\n",
        "        lambda_comm_eff = config.lambda_comm * ramp\n",
        "\n",
        "        # Collect episode\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        ep_kappa_list = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon, use_projection=True)\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = agent.forward(torch.FloatTensor(obs).unsqueeze(0), use_projection=True)\n",
        "                ep_kappa_list.append(out['kappa'].item())\n",
        "\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "        ep_kappas.append(float(np.mean(ep_kappa_list)))\n",
        "        ep_dists.append(info['distance'])\n",
        "\n",
        "        # Training updates\n",
        "        if len(buffer) >= config.batch_size:\n",
        "            batch = buffer.sample(config.batch_size)\n",
        "\n",
        "            # Main update\n",
        "            losses_main = compute_main_loss(\n",
        "                agent, **batch,\n",
        "                lambda_cons=config.lambda_cons,\n",
        "                entropy_coef=config.entropy_coef,\n",
        "                use_projection=True\n",
        "            )\n",
        "\n",
        "            optimizer_main.zero_grad()\n",
        "            losses_main['total_loss'].backward()\n",
        "            torch.nn.utils.clip_grad_norm_(main_params, config.grad_clip)\n",
        "            optimizer_main.step()\n",
        "\n",
        "            # Projector update (less frequent)\n",
        "            if ramp > 0 and (episode + 1) % config.proj_update_every == 0:\n",
        "                losses_proj = compute_proj_loss(\n",
        "                    agent, **batch,\n",
        "                    lambda_kappa=lambda_kappa_eff,\n",
        "                    lambda_comm=lambda_comm_eff,\n",
        "                    adv_weight=config.adv_weight_kappa,\n",
        "                    adv_temp=config.adv_temp\n",
        "                )\n",
        "\n",
        "                optimizer_proj.zero_grad()\n",
        "                losses_proj['total_loss'].backward()\n",
        "                torch.nn.utils.clip_grad_norm_(proj_params, config.grad_clip * 0.5)\n",
        "                optimizer_proj.step()\n",
        "\n",
        "                comm_norms.append(losses_proj['L_comm'].item())\n",
        "\n",
        "            agent.update_value_target()\n",
        "\n",
        "        epsilon = max(config.epsilon_end, epsilon * config.epsilon_decay)\n",
        "\n",
        "        # Logging\n",
        "        if (episode + 1) % print_every == 0:\n",
        "            avg_r = float(np.mean(ep_rewards[-print_every:]))\n",
        "            avg_k = float(np.mean(ep_kappas[-print_every:]))\n",
        "            avg_d = float(np.mean(ep_dists[-print_every:]))\n",
        "            eff_rank = agent.projector.get_effective_rank()\n",
        "\n",
        "            print(f\"Ep {episode+1:4d} | R: {avg_r:6.2f} | κ: {avg_k:.3f} | \"\n",
        "                  f\"D: {avg_d:.3f} | ε: {epsilon:.3f} | ramp: {ramp:.2f} | eff_rank: {eff_rank:.1f}\")\n",
        "\n",
        "            if len(buffer) >= config.batch_size:\n",
        "                print(f\"          | L_RL: {losses_main['L_RL'].item():.4f} | \"\n",
        "                      f\"L_cons: {losses_main['L_cons'].item():.4f} | \"\n",
        "                      f\"H: {losses_main['entropy'].item():.3f}\")\n",
        "                if 'losses_proj' in locals():\n",
        "                    print(f\"          | L_rel: {losses_proj['L_rel'].item():.4f} | \"\n",
        "                          f\"L_comm: {losses_proj['L_comm'].item():.6f}\")\n",
        "\n",
        "    # Final analysis\n",
        "    k_tail = np.array(ep_kappas[-50:])\n",
        "    r_tail = np.array(ep_rewards[-50:])\n",
        "    corr = np.corrcoef(k_tail, r_tail)[0, 1] if len(k_tail) > 1 else 0.0\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Training Complete\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Final κ (last 50): {np.mean(k_tail):.3f} ± {np.std(k_tail):.3f}\")\n",
        "    print(f\"Final reward (last 50): {np.mean(r_tail):.2f} ± {np.std(r_tail):.2f}\")\n",
        "    print(f\"Final distance (last 50): {np.mean(ep_dists[-50:]):.3f}\")\n",
        "    print(f\"Corr(κ, Return): {corr:.3f}\")\n",
        "    if comm_norms:\n",
        "        print(f\"Final commutator norm: {np.mean(comm_norms[-10:]):.6f}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return agent, ep_rewards, ep_kappas, ep_dists\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Ablations\n",
        "# ============================================================================\n",
        "\n",
        "def run_ablation_a1(config: STCConfig, num_episodes: int = 100) -> float:\n",
        "    \"\"\"A1: No projector (full latent space)\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ABLATION A1: No Projector\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    env = SimpleControlEnv(progress_bonus=config.progress_bonus)\n",
        "    agent = STCAlbertaAgent(config)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=config.lr_main)\n",
        "    buffer = ReplayBuffer()\n",
        "\n",
        "    ep_rewards = []\n",
        "    epsilon = 0.1\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon, use_projection=False)\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= config.batch_size:\n",
        "            batch = buffer.sample(config.batch_size)\n",
        "            losses = compute_main_loss(agent, **batch, lambda_cons=config.lambda_cons,\n",
        "                                      entropy_coef=config.entropy_coef, use_projection=False)\n",
        "            optimizer.zero_grad()\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    result = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"A1 Result (last 30): {result:.2f}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_ablation_a2(config: STCConfig, num_episodes: int = 100) -> float:\n",
        "    \"\"\"A2: Random fixed projector\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ABLATION A2: Random Fixed Projector\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    env = SimpleControlEnv(progress_bonus=config.progress_bonus)\n",
        "    agent = STCAlbertaAgent(config)\n",
        "\n",
        "    # Freeze projector with random initialization\n",
        "    for p in agent.projector.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.Adam([p for p in agent.parameters() if p.requires_grad],\n",
        "                                 lr=config.lr_main)\n",
        "    buffer = ReplayBuffer()\n",
        "\n",
        "    ep_rewards = []\n",
        "    epsilon = 0.1\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon, use_projection=True)\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= config.batch_size:\n",
        "            batch = buffer.sample(config.batch_size)\n",
        "            losses = compute_main_loss(agent, **batch, lambda_cons=config.lambda_cons,\n",
        "                                      entropy_coef=config.entropy_coef, use_projection=True)\n",
        "            optimizer.zero_grad()\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    result = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"A2 Result (last 30): {result:.2f}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_ablation_a3(config: STCConfig, num_episodes: int = 100) -> float:\n",
        "    \"\"\"A3: Learned projector but no commutator penalty\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ABLATION A3: No Commutator Penalty\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    config_a3 = STCConfig()\n",
        "    config_a3.__dict__.update(config.__dict__)\n",
        "    config_a3.lambda_comm = 0.0  # Disable commutator\n",
        "    config_a3.num_episodes = num_episodes\n",
        "\n",
        "    _, _, _, _ = train_stc_alberta(config_a3, print_every=20)\n",
        "\n",
        "    # Return is printed in train function\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = STCConfig()\n",
        "\n",
        "    # Main training\n",
        "    agent, rewards, kappas, dists = train_stc_alberta(config, print_every=10)\n",
        "\n",
        "    # Run ablations\n",
        "    a1_result = run_ablation_a1(config, num_episodes=100)\n",
        "    a2_result = run_ablation_a2(config, num_episodes=100)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FINAL COMPARISON\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"STC-Alberta (full):        {np.mean(rewards[-50:]):.2f}\")\n",
        "    print(f\"A1 (no projector):         {a1_result:.2f}\")\n",
        "    print(f\"A2 (random projector):     {a2_result:.2f}\")\n",
        "    print(f\"Improvement over A1:       {np.mean(rewards[-50:]) - a1_result:.2f}\")\n",
        "    print(f\"Improvement over A2:       {np.mean(rewards[-50:]) - a2_result:.2f}\")\n",
        "    print(\"=\" * 70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOHbbI-hzPMR",
        "outputId": "ef1385b8-86c4-49f8-9817-902b8e536994"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STC-Alberta Agent V2.0 Training\n",
            "======================================================================\n",
            "Config: 200 episodes, rank 16/64\n",
            "======================================================================\n",
            "\n",
            "Ep   10 | R: -326.91 | κ: 0.269 | D: 1.468 | ε: 0.461 | ramp: 0.00 | eff_rank: 16.0\n",
            "          | L_RL: 1.8580 | L_cons: 2.3328 | H: 1.384\n",
            "Ep   20 | R: -276.90 | κ: 0.400 | D: 1.456 | ε: 0.426 | ramp: 0.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.3901 | L_cons: 1.5106 | H: 1.381\n",
            "Ep   30 | R: -219.47 | κ: 0.514 | D: 0.992 | ε: 0.393 | ramp: 0.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.1250 | L_cons: 0.9646 | H: 1.377\n",
            "Ep   40 | R: -252.18 | κ: 0.525 | D: 1.138 | ε: 0.363 | ramp: 0.11 | eff_rank: 16.0\n",
            "          | L_RL: 0.1573 | L_cons: 0.5367 | H: 1.376\n",
            "          | L_rel: -0.5779 | L_comm: 0.000027\n",
            "Ep   50 | R: -184.64 | κ: 0.598 | D: 0.735 | ε: 0.335 | ramp: 0.24 | eff_rank: 16.0\n",
            "          | L_RL: 0.1053 | L_cons: 0.4001 | H: 1.374\n",
            "          | L_rel: -0.6093 | L_comm: 0.000027\n",
            "Ep   60 | R: -284.37 | κ: 0.625 | D: 1.404 | ε: 0.309 | ramp: 0.36 | eff_rank: 16.0\n",
            "          | L_RL: 0.1423 | L_cons: 0.3001 | H: 1.368\n",
            "          | L_rel: -0.6284 | L_comm: 0.000028\n",
            "Ep   70 | R: -222.61 | κ: 0.632 | D: 1.092 | ε: 0.285 | ramp: 0.49 | eff_rank: 16.0\n",
            "          | L_RL: 0.1036 | L_cons: 0.3396 | H: 1.374\n",
            "          | L_rel: -0.6235 | L_comm: 0.000029\n",
            "Ep   80 | R: -223.38 | κ: 0.642 | D: 1.259 | ε: 0.263 | ramp: 0.61 | eff_rank: 16.0\n",
            "          | L_RL: 0.0466 | L_cons: 0.3599 | H: 1.377\n",
            "          | L_rel: -0.6321 | L_comm: 0.000032\n",
            "Ep   90 | R: -173.73 | κ: 0.640 | D: 0.937 | ε: 0.243 | ramp: 0.74 | eff_rank: 16.0\n",
            "          | L_RL: 0.0220 | L_cons: 0.3321 | H: 1.376\n",
            "          | L_rel: -0.6423 | L_comm: 0.000032\n",
            "Ep  100 | R: -210.70 | κ: 0.641 | D: 1.038 | ε: 0.224 | ramp: 0.86 | eff_rank: 16.0\n",
            "          | L_RL: 0.0471 | L_cons: 0.2798 | H: 1.379\n",
            "          | L_rel: -0.6561 | L_comm: 0.000032\n",
            "Ep  110 | R: -223.82 | κ: 0.670 | D: 0.987 | ε: 0.207 | ramp: 0.99 | eff_rank: 16.0\n",
            "          | L_RL: 0.0230 | L_cons: 0.3777 | H: 1.373\n",
            "          | L_rel: -0.6570 | L_comm: 0.000031\n",
            "Ep  120 | R: -221.73 | κ: 0.633 | D: 1.209 | ε: 0.191 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0391 | L_cons: 0.2459 | H: 1.373\n",
            "          | L_rel: -0.6695 | L_comm: 0.000030\n",
            "Ep  130 | R: -298.92 | κ: 0.668 | D: 1.679 | ε: 0.176 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0676 | L_cons: 0.3437 | H: 1.373\n",
            "          | L_rel: -0.6488 | L_comm: 0.000030\n",
            "Ep  140 | R: -258.34 | κ: 0.663 | D: 1.315 | ε: 0.162 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0429 | L_cons: 0.2135 | H: 1.367\n",
            "          | L_rel: -0.6783 | L_comm: 0.000030\n",
            "Ep  150 | R: -262.29 | κ: 0.682 | D: 1.419 | ε: 0.150 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0352 | L_cons: 0.2698 | H: 1.368\n",
            "          | L_rel: -0.6852 | L_comm: 0.000030\n",
            "Ep  160 | R: -107.23 | κ: 0.567 | D: 0.666 | ε: 0.138 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0333 | L_cons: 0.2233 | H: 1.366\n",
            "          | L_rel: -0.6465 | L_comm: 0.000031\n",
            "Ep  170 | R: -141.84 | κ: 0.600 | D: 0.588 | ε: 0.128 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0831 | L_cons: 0.2108 | H: 1.357\n",
            "          | L_rel: -0.6142 | L_comm: 0.000031\n",
            "Ep  180 | R: -133.24 | κ: 0.583 | D: 0.828 | ε: 0.118 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0128 | L_cons: 0.1998 | H: 1.358\n",
            "          | L_rel: -0.6442 | L_comm: 0.000032\n",
            "Ep  190 | R: -149.40 | κ: 0.598 | D: 0.524 | ε: 0.109 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0383 | L_cons: 0.1649 | H: 1.355\n",
            "          | L_rel: -0.6670 | L_comm: 0.000032\n",
            "Ep  200 | R: -203.45 | κ: 0.616 | D: 1.239 | ε: 0.100 | ramp: 1.00 | eff_rank: 16.0\n",
            "          | L_RL: 0.0690 | L_cons: 0.1688 | H: 1.350\n",
            "          | L_rel: -0.6198 | L_comm: 0.000033\n",
            "\n",
            "======================================================================\n",
            "Training Complete\n",
            "======================================================================\n",
            "Final κ (last 50): 0.593 ± 0.074\n",
            "Final reward (last 50): -147.03 ± 90.80\n",
            "Final distance (last 50): 0.769\n",
            "Corr(κ, Return): -0.349\n",
            "Final commutator norm: 0.000032\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ABLATION A1: No Projector\n",
            "======================================================================\n",
            "A1 Result (last 30): -204.80\n",
            "\n",
            "======================================================================\n",
            "ABLATION A2: Random Fixed Projector\n",
            "======================================================================\n",
            "A2 Result (last 30): -247.25\n",
            "\n",
            "======================================================================\n",
            "FINAL COMPARISON\n",
            "======================================================================\n",
            "STC-Alberta (full):        -147.03\n",
            "A1 (no projector):         -204.80\n",
            "A2 (random projector):     -247.25\n",
            "Improvement over A1:       57.77\n",
            "Improvement over A2:       100.22\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}