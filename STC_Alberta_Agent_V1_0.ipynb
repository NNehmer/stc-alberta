{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdSqGnscW0dEUh5m+CZbLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NNehmer/stc-alberta/blob/main/STC_Alberta_Agent_V1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install --upgrade torch"
      ],
      "metadata": {
        "id": "nBR29Jz70jWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, sys\n",
        "print(\"Torch:\", torch.__version__, \"Python:\", sys.version.split()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_j3V3MK6U7O",
        "outputId": "4ba993b1-78f3-4c5c-a351-47e2134c07f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 Python: 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "STC–Alberta Agent — Colab-sichere Version (nur Definitionen)\n",
        "- konsistente Einrückung (4 Spaces)\n",
        "- keine Tabs\n",
        "- kein automatischer Startblock\n",
        "- imagine() akzeptiert Index- oder One-Hot-Aktionen\n",
        "- Entropie-Bonus, EMA-Target, korrekter Kommutator, saubere Logs\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) STC Bausteine\n",
        "# ------------------------------------------------------------\n",
        "class SpectralProjector(nn.Module):\n",
        "    \"\"\"Intentionaler Subraum-Projektor Π_S (QR-orthonormalisiert)\"\"\"\n",
        "    def __init__(self, latent_dim: int, intent_rank: int):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "        self.basis = nn.Parameter(torch.randn(latent_dim, intent_rank) * 0.1)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        Q, _ = torch.linalg.qr(self.basis)   # [D, r]\n",
        "        Pi_S = Q @ Q.T                       # [D, D]\n",
        "        z_S = z @ Pi_S                       # Projektion\n",
        "        return z_S, Pi_S\n",
        "\n",
        "def coherence(z: torch.Tensor, z_S: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"κ(ψ) = ||Π_S ψ||² / ||ψ||²  (formstabil, keepdim=True)\"\"\"\n",
        "    norm_z   = torch.norm(z,   dim=-1, keepdim=True) + 1e-8\n",
        "    norm_z_S = torch.norm(z_S, dim=-1, keepdim=True)\n",
        "    return (norm_z_S / norm_z) ** 2\n",
        "\n",
        "class ValueOperator(nn.Module):\n",
        "    \"\"\"Symmetrischer Wertoperator V, v(ψ)=⟨ψ|V|ψ⟩\"\"\"\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(latent_dim, latent_dim) * 0.01)\n",
        "\n",
        "    def forward(self, z_S: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        V = 0.5 * (self.W + self.W.T)\n",
        "        v = torch.einsum('bi,ij,bj->b', z_S, V, z_S)  # ⟨ψ|V|ψ⟩\n",
        "        return v.unsqueeze(-1), V\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Agent\n",
        "# ------------------------------------------------------------\n",
        "class STCAlbertaAgent(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int, latent_dim: int = 64, intent_rank: int = 16):\n",
        "        super().__init__()\n",
        "        self.obs_dim     = obs_dim\n",
        "        self.action_dim  = action_dim\n",
        "        self.latent_dim  = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128), nn.LayerNorm(128), nn.ReLU(),\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.projector  = SpectralProjector(latent_dim, intent_rank)\n",
        "        self.value_op   = ValueOperator(latent_dim)\n",
        "        self.value_target = ValueOperator(latent_dim)        # EMA-Target\n",
        "        self.value_target.load_state_dict(self.value_op.state_dict())\n",
        "        self.value_tau = 0.005\n",
        "\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128), nn.ReLU(), nn.Linear(128, action_dim)\n",
        "        )\n",
        "        self.transition = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 128), nn.ReLU(), nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.reward_head = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_value_target(self):\n",
        "        for p, pt in zip(self.value_op.parameters(), self.value_target.parameters()):\n",
        "            pt.data.mul_(1 - self.value_tau).add_(self.value_tau * p.data)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        z      = self.encoder(obs)\n",
        "        z_S, P = self.projector(z)\n",
        "        kappa  = coherence(z, z_S)\n",
        "        value, V = self.value_op(z_S)\n",
        "        logits = self.policy(z_S)\n",
        "        return {\n",
        "            'latent': z, 'latent_S': z_S, 'projector': P, 'value_matrix': V,\n",
        "            'kappa': kappa, 'value': value, 'logits': logits\n",
        "        }\n",
        "\n",
        "    def imagine(self, z_S: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"actions: [B] (Indices) oder [B,A] (One-Hot)\"\"\"\n",
        "        if actions.dim() == 1:\n",
        "            a_oh = F.one_hot(actions.long(), num_classes=self.action_dim).float()\n",
        "        elif actions.dim() == 2 and actions.size(-1) == self.action_dim:\n",
        "            a_oh = actions.float()\n",
        "        else:\n",
        "            raise ValueError(\"actions must be [B] (indices) or [B,A] (one-hot)\")\n",
        "        za = torch.cat([z_S, a_oh], dim=-1)\n",
        "        z_S_next = self.transition(za)\n",
        "        reward   = self.reward_head(za)\n",
        "        return z_S_next, reward\n",
        "\n",
        "    def select_action(self, obs: torch.Tensor, epsilon: float = 0.0) -> int:\n",
        "        with torch.no_grad():\n",
        "            out = self.forward(obs.unsqueeze(0))\n",
        "            if random.random() < epsilon:\n",
        "                return random.randint(0, self.action_dim - 1)\n",
        "            return out['logits'].argmax(dim=-1).item()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Verluste\n",
        "# ------------------------------------------------------------\n",
        "def stc_loss(\n",
        "    agent: STCAlbertaAgent,\n",
        "    obs: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor,\n",
        "    next_obs: torch.Tensor, dones: torch.Tensor,\n",
        "    gamma: float = 0.99, lambda_kappa: float = 0.1,\n",
        "    lambda_comm: float = 0.01, lambda_cons: float = 0.5\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    out      = agent(obs)\n",
        "    out_next = agent(next_obs)\n",
        "\n",
        "    z_S, P, V = out['latent_S'], out['projector'], out['value_matrix']\n",
        "    kappa, value, logits = out['kappa'], out['value'], out['logits']\n",
        "\n",
        "    dist  = torch.distributions.Categorical(logits=logits)\n",
        "    logp  = dist.log_prob(actions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        v_next_target, _ = agent.value_target(out_next['latent_S'])\n",
        "        td_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "\n",
        "    td_err      = td_target - value\n",
        "    policy_loss = -(logp * td_err.squeeze(-1).detach()).mean()\n",
        "    value_loss  = F.mse_loss(value, td_target)\n",
        "    entropy     = dist.entropy().mean()\n",
        "    L_RL        = policy_loss + value_loss - 1e-3 * entropy\n",
        "\n",
        "    L_rel  = -kappa.mean()\n",
        "    comm   = V @ P - P @ V\n",
        "    L_comm = (torch.norm(comm, p='fro') ** 2) / V.numel()\n",
        "\n",
        "    z_S_next_pred, reward_pred = agent.imagine(z_S, actions)\n",
        "    with torch.no_grad():\n",
        "        v_next_pred_target, _ = agent.value_target(z_S_next_pred)\n",
        "    q_pred   = reward_pred + gamma * v_next_pred_target * (1 - dones.unsqueeze(-1))\n",
        "    q_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "    L_cons   = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "    total = L_RL + lambda_kappa * L_rel + lambda_comm * L_comm + lambda_cons * L_cons\n",
        "    return {\n",
        "        'total_loss': total, 'L_RL': L_RL, 'L_rel': L_rel, 'L_comm': L_comm, 'L_cons': L_cons,\n",
        "        'kappa': kappa.mean(), 'td_error': td_err.abs().mean(), 'policy_loss': policy_loss,\n",
        "        'value_loss': value_loss, 'entropy': entropy\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Einfache 2D-Nav-Umwelt (diskrete Aktionen)\n",
        "# ------------------------------------------------------------\n",
        "class SimpleControlEnv:\n",
        "    \"\"\"State: [x,y,goal_x,goal_y,vx,vy], Actions: up/down/left/right\"\"\"\n",
        "    def __init__(self):\n",
        "        self.state_dim  = 6\n",
        "        self.action_dim = 4\n",
        "        self.max_steps  = 200\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        self.pos   = np.random.uniform(-1, 1, 2)\n",
        "        self.vel   = np.zeros(2)\n",
        "        self.goal  = np.random.uniform(-1, 1, 2)\n",
        "        self.steps = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        return np.concatenate([self.pos, self.goal, self.vel])\n",
        "\n",
        "    def step(self, action: int):\n",
        "        acc_map = {0: np.array([0, 0.1]), 1: np.array([0, -0.1]),\n",
        "                   2: np.array([-0.1, 0]), 3: np.array([0.1, 0])}\n",
        "        acc  = acc_map[action]\n",
        "        self.vel = np.clip(0.9 * self.vel + acc, -0.5, 0.5)\n",
        "        self.pos = np.clip(self.pos + self.vel, -1,  1)\n",
        "        dist = float(np.linalg.norm(self.pos - self.goal))\n",
        "        reward = -dist\n",
        "        self.steps += 1\n",
        "        done = (dist < 0.1) or (self.steps >= self.max_steps)\n",
        "        return self._get_obs(), reward, done, {'distance': dist}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Replay Buffer\n",
        "# ------------------------------------------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, obs, action, reward, next_obs, done):\n",
        "        self.buffer.append((obs, action, reward, next_obs, done))\n",
        "\n",
        "    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
        "        return {\n",
        "            'obs':      torch.FloatTensor(np.array(obs)),\n",
        "            'actions':  torch.LongTensor(actions),\n",
        "            'rewards':  torch.FloatTensor(rewards),\n",
        "            'next_obs': torch.FloatTensor(np.array(next_obs)),\n",
        "            'dones':    torch.FloatTensor(dones)\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Training (keine Ausführung hier)\n",
        "# ------------------------------------------------------------\n",
        "def train_stc_alberta(\n",
        "    num_episodes: int = 60,\n",
        "    batch_size: int = 64,\n",
        "    buffer_size: int = 10000,\n",
        "    learning_rate: float = 3e-4,\n",
        "    epsilon_start: float = 1.0,\n",
        "    epsilon_end: float = 0.05,\n",
        "    epsilon_decay: float = 0.995,\n",
        "    print_every: int = 10\n",
        "):\n",
        "    env      = SimpleControlEnv()\n",
        "    agent    = STCAlbertaAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer= torch.optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "    buffer   = ReplayBuffer(capacity=buffer_size)\n",
        "\n",
        "    epsilon  = epsilon_start\n",
        "    ep_rewards, ep_kappas, ep_dists = [], [], []\n",
        "\n",
        "    # Header\n",
        "    print(\"=\"*70)\n",
        "    print(\"STC–Alberta Agent Training\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Environment: SimpleControlEnv (2D)\")\n",
        "    print(f\"State dim: {env.state_dim}, Action dim: {env.action_dim}\")\n",
        "    print(\"Agent latent dim: 64, Intent rank: 16\")\n",
        "    print(\"=\"*70)\n",
        "    print()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        ep_k_list: List[float] = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon)\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = agent(torch.FloatTensor(obs).unsqueeze(0))\n",
        "                ep_k_list.append(out['kappa'].item())\n",
        "\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "        ep_kappas.append(float(np.mean(ep_k_list)))\n",
        "        ep_dists.append(info['distance'])\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = buffer.sample(batch_size)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch)\n",
        "            losses['total_loss'].backward()\n",
        "            torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "        if (episode + 1) % print_every == 0:\n",
        "            avgR = float(np.mean(ep_rewards[-print_every:]))\n",
        "            avgK = float(np.mean(ep_kappas[-print_every:]))\n",
        "            avgD = float(np.mean(ep_dists[-print_every:]))\n",
        "            msg  = (f\"Episode {episode+1:4d} | Reward: {avgR:7.2f} | κ: {avgK:.3f} \"\n",
        "                    f\"| Dist: {avgD:.3f} | ε: {epsilon:.3f}\")\n",
        "            if len(buffer) >= batch_size:\n",
        "                msg += (f\"\\n             | L_RL: {losses['L_RL'].item():.4f} | L_rel: {losses['L_rel'].item():.4f} \"\n",
        "                        f\"| L_comm: {losses['L_comm'].item():.6f} | L_cons: {losses['L_cons'].item():.4f} \"\n",
        "                        f\"| H: {losses['entropy'].item():.3f}\")\n",
        "            print(msg)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*70)\n",
        "    print(\"Training Complete\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Final κ (last 30): {np.mean(ep_kappas[-30:]):.3f}\")\n",
        "    print(f\"Final reward (last 30): {np.mean(ep_rewards[-30:]):.2f}\")\n",
        "    print(f\"Final distance (last 30): {np.mean(ep_dists[-30:]):.3f}\")\n",
        "\n",
        "    return agent, ep_rewards, ep_kappas, ep_dists\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Ablation (A1: ohne Π_S)\n",
        "# ------------------------------------------------------------\n",
        "def run_ablation_no_projector(num_episodes: int = 60):\n",
        "    print(\"\\n\"+\"=\"*70)\n",
        "    print(\"ABLATION A1: No Projector (Full Latent Space)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    class NoProjAgent(STCAlbertaAgent):\n",
        "        def forward(self, obs):\n",
        "            z   = self.encoder(obs)\n",
        "            z_S = z\n",
        "            P   = torch.eye(self.latent_dim, device=z.device)\n",
        "            k   = torch.ones(z.shape[0], 1, device=z.device)\n",
        "            v, V = self.value_op(z_S)\n",
        "            logits = self.policy(z_S)\n",
        "            return {'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "                    'value_matrix': V, 'kappa': k, 'value': v, 'logits': logits}\n",
        "\n",
        "    env = SimpleControlEnv()\n",
        "    agent = NoProjAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=3e-4)\n",
        "    buffer = ReplayBuffer()\n",
        "    ep_rewards: List[float] = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon=0.1)\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= 64:\n",
        "            batch = buffer.sample(64)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch, lambda_kappa=0.0)  # ohne κ-Reg.\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    print(f\"Final reward (A1, no Π_S, last 30): {np.mean(ep_rewards[-30:]):.2f}\")\n",
        "    return float(np.mean(ep_rewards[-30:]))\n"
      ],
      "metadata": {
        "id": "yguBAz7H6aFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kurzer Lauf (Colab-freundlich); du kannst num_episodes später erhöhen.\n",
        "agent, rewards, kappas, dists = train_stc_alberta(\n",
        "    num_episodes=50,   # klein halten für schnellen Start\n",
        "    batch_size=64,\n",
        "    learning_rate=3e-4,\n",
        "    print_every=10\n",
        ")\n",
        "\n",
        "ablation_reward = run_ablation_no_projector(num_episodes=50)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"STC–Alberta (with Π_S): {np.mean(rewards[-30:]):.2f} (last 30 eps)\")\n",
        "print(f\"Ablation A1 (no Π_S):   {ablation_reward:.2f} (last 30 eps)\")\n",
        "print(f\"Improvement:            {np.mean(rewards[-30:]) - ablation_reward:.2f}\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGRepVyZ6slH",
        "outputId": "e3b59417-06a9-4606-86d6-bb7c5065863f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STC–Alberta Agent Training\n",
            "======================================================================\n",
            "Environment: SimpleControlEnv (2D)\n",
            "State dim: 6, Action dim: 4\n",
            "Agent latent dim: 64, Intent rank: 16\n",
            "======================================================================\n",
            "\n",
            "Episode   10 | Reward: -262.87 | κ: 0.356 | Dist: 0.820 | ε: 0.951\n",
            "             | L_RL: -0.0334 | L_rel: -0.4398 | L_comm: 0.000020 | L_cons: 2.0184 | H: 1.375\n",
            "Episode   20 | Reward: -255.37 | κ: 0.550 | Dist: 1.188 | ε: 0.905\n",
            "             | L_RL: 1.5352 | L_rel: -0.6049 | L_comm: 0.000022 | L_cons: 1.6860 | H: 1.357\n",
            "Episode   30 | Reward: -286.02 | κ: 0.618 | Dist: 1.526 | ε: 0.860\n",
            "             | L_RL: 0.0220 | L_rel: -0.5992 | L_comm: 0.000023 | L_cons: 1.7142 | H: 1.353\n",
            "Episode   40 | Reward: -239.62 | κ: 0.653 | Dist: 1.109 | ε: 0.818\n",
            "             | L_RL: 0.7786 | L_rel: -0.6725 | L_comm: 0.000023 | L_cons: 0.8779 | H: 1.347\n",
            "Episode   50 | Reward: -237.62 | κ: 0.695 | Dist: 1.132 | ε: 0.778\n",
            "             | L_RL: 0.0917 | L_rel: -0.7121 | L_comm: 0.000024 | L_cons: 0.7995 | H: 1.342\n",
            "\n",
            "======================================================================\n",
            "Training Complete\n",
            "======================================================================\n",
            "Final κ (last 30): 0.655\n",
            "Final reward (last 30): -254.42\n",
            "Final distance (last 30): 1.255\n",
            "\n",
            "======================================================================\n",
            "ABLATION A1: No Projector (Full Latent Space)\n",
            "======================================================================\n",
            "Final reward (A1, no Π_S, last 30): -262.25\n",
            "\n",
            "======================================================================\n",
            "RESULTS SUMMARY\n",
            "======================================================================\n",
            "STC–Alberta (with Π_S): -254.42 (last 30 eps)\n",
            "Ablation A1 (no Π_S):   -262.25 (last 30 eps)\n",
            "Improvement:            7.83\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}