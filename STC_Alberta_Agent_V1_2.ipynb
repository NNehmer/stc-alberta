{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUw5GfPX8EXqZpnjhi41TA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NNehmer/stc-alberta/blob/main/STC_Alberta_Agent_V1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install --upgrade torch"
      ],
      "metadata": {
        "id": "nBR29Jz70jWn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, sys\n",
        "print(\"Torch:\", torch.__version__, \"Python:\", sys.version.split()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_j3V3MK6U7O",
        "outputId": "2cb1b508-7fa3-475d-8fdb-b9d9aa47abc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 Python: 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: aktuelle Torch-Version\n",
        "# %pip -q install --upgrade torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "# ---------------------------\n",
        "# Reproduzierbarkeit\n",
        "# ---------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# ==========================================\n",
        "# 1) STC-Bausteine\n",
        "# ==========================================\n",
        "class SpectralProjector(nn.Module):\n",
        "    \"\"\"Intentionaler Subraum-Projektor Π_S (QR-orthonormalisiert).\"\"\"\n",
        "    def __init__(self, latent_dim: int, intent_rank: int):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "        self.basis = nn.Parameter(torch.randn(latent_dim, intent_rank) * 0.1)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        Q, _ = torch.linalg.qr(self.basis)    # [D, r], orthonormal\n",
        "        Pi_S = Q @ Q.T                        # [D, D]\n",
        "        z_S  = z @ Pi_S\n",
        "        return z_S, Pi_S\n",
        "\n",
        "def coherence(z: torch.Tensor, z_S: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"κ(ψ) = ||Π_S ψ||² / ||ψ||²  (mit keepdim=True).\"\"\"\n",
        "    nz  = torch.norm(z,   dim=-1, keepdim=True) + 1e-8\n",
        "    nzS = torch.norm(z_S, dim=-1, keepdim=True)\n",
        "    return (nzS / nz) ** 2\n",
        "\n",
        "class ValueOperator(nn.Module):\n",
        "    \"\"\"Symmetrischer Wertoperator V; v(ψ)=⟨ψ|V|ψ⟩.\"\"\"\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(latent_dim, latent_dim) * 0.01)\n",
        "\n",
        "    def forward(self, z_S: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        V = 0.5 * (self.W + self.W.T)\n",
        "        v = torch.einsum('bi,ij,bj->b', z_S, V, z_S)\n",
        "        return v.unsqueeze(-1), V\n",
        "\n",
        "# ==========================================\n",
        "# 2) Agent mit Policy-Mix & optionalem Concat-Head\n",
        "# ==========================================\n",
        "class STCAlbertaAgent(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int, latent_dim: int = 64, intent_rank: int = 16):\n",
        "        super().__init__()\n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128), nn.LayerNorm(128), nn.ReLU(),\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.projector = SpectralProjector(latent_dim, intent_rank)\n",
        "        self.value_op  = ValueOperator(latent_dim)\n",
        "\n",
        "        # EMA-Target für TD-Stabilität\n",
        "        self.value_target = ValueOperator(latent_dim)\n",
        "        self.value_target.load_state_dict(self.value_op.state_dict())\n",
        "        self.value_tau = 0.005\n",
        "\n",
        "        # Policy \"mix\": gleiche Dim wie z/z_S\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128), nn.ReLU(), nn.Linear(128, action_dim)\n",
        "        )\n",
        "        # Policy \"concat\": optionaler Head auf [z, z_S] (2*latent)\n",
        "        self.policy_concat = nn.Sequential(\n",
        "            nn.Linear(2*latent_dim, 128), nn.ReLU(), nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "        # Dynamik & Reward im z_S-Raum\n",
        "        self.transition = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 128), nn.ReLU(), nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.reward_head = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_value_target(self):\n",
        "        for p, pt in zip(self.value_op.parameters(), self.value_target.parameters()):\n",
        "            pt.data.mul_(1 - self.value_tau).add_(self.value_tau * p.data)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        obs: torch.Tensor,\n",
        "        mix_alpha: float = 1.0,          # 0 → reine z, 1 → reine z_S\n",
        "        policy_mode: str = \"mix\"         # \"mix\", \"concat\" oder \"zs\" (nur z_S)\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        z      = self.encoder(obs)       # [B,D]\n",
        "        z_S, P = self.projector(z)       # [B,D], [D,D]\n",
        "        kappa  = coherence(z, z_S)       # [B,1]\n",
        "        value, V = self.value_op(z_S)    # [B,1], [D,D]\n",
        "\n",
        "        if policy_mode == \"mix\":\n",
        "            feat   = (1.0 - mix_alpha) * z + mix_alpha * z_S\n",
        "            logits = self.policy(feat)\n",
        "        elif policy_mode == \"concat\":\n",
        "            feat   = torch.cat([z, z_S], dim=-1)\n",
        "            logits = self.policy_concat(feat)\n",
        "        elif policy_mode == \"zs\":\n",
        "            logits = self.policy(z_S)\n",
        "        else:\n",
        "            logits = self.policy(z_S)\n",
        "\n",
        "        return {\n",
        "            'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "            'value_matrix': V, 'kappa': kappa, 'value': value, 'logits': logits\n",
        "        }\n",
        "\n",
        "    def imagine(self, z_S: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"actions: [B] (indices) oder [B,A] (one-hot).\"\"\"\n",
        "        if actions.dim() == 1:\n",
        "            a_oh = F.one_hot(actions.long(), num_classes=self.action_dim).float()\n",
        "        elif actions.dim() == 2 and actions.size(-1) == self.action_dim:\n",
        "            a_oh = actions.float()\n",
        "        else:\n",
        "            raise ValueError(\"actions must be [B] (indices) or [B,A] (one-hot)\")\n",
        "        za = torch.cat([z_S, a_oh], dim=-1)\n",
        "        z_S_next = self.transition(za)\n",
        "        reward   = self.reward_head(za)\n",
        "        return z_S_next, reward\n",
        "\n",
        "    def select_action(self, obs: torch.Tensor, epsilon: float = 0.0, mix_alpha: float = 1.0, policy_mode: str = \"mix\") -> int:\n",
        "        with torch.no_grad():\n",
        "            out = self.forward(obs.unsqueeze(0), mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "            if random.random() < epsilon:\n",
        "                return random.randint(0, self.action_dim - 1)\n",
        "            return out['logits'].argmax(dim=-1).item()\n",
        "\n",
        "# ==========================================\n",
        "# 3) Verlustfunktion (parametrisierbar + Policy-Mix)\n",
        "# ==========================================\n",
        "def stc_loss(\n",
        "    agent: STCAlbertaAgent,\n",
        "    obs: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor,\n",
        "    next_obs: torch.Tensor, dones: torch.Tensor,\n",
        "    gamma: float = 0.99,\n",
        "    lambda_kappa: float = 0.1,\n",
        "    lambda_comm: float = 0.01,\n",
        "    lambda_cons: float = 1.0,\n",
        "    entropy_coef: float = 5e-4,\n",
        "    # neu:\n",
        "    mix_alpha: float = 1.0,\n",
        "    policy_mode: str = \"mix\",\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "\n",
        "    out      = agent(obs,      mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "    out_next = agent(next_obs, mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "\n",
        "    z_S, P, V     = out['latent_S'], out['projector'], out['value_matrix']\n",
        "    kappa         = out['kappa']\n",
        "    value, logits = out['value'], out['logits']\n",
        "\n",
        "    dist  = torch.distributions.Categorical(logits=logits)\n",
        "    logp  = dist.log_prob(actions)\n",
        "\n",
        "    # TD-Target mit EMA-Target\n",
        "    with torch.no_grad():\n",
        "        v_next_target, _ = agent.value_target(out_next['latent_S'])\n",
        "        td_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "\n",
        "    td_err      = td_target - value\n",
        "    policy_loss = -(logp * td_err.squeeze(-1).detach()).mean()\n",
        "    value_loss  = F.mse_loss(value, td_target)\n",
        "    entropy     = dist.entropy().mean()\n",
        "    L_RL        = policy_loss + value_loss - entropy_coef * entropy\n",
        "\n",
        "    # Relevanz-Maximierung\n",
        "    L_rel  = -kappa.mean()\n",
        "\n",
        "    # Kommutator-Penalty (Frobenius, skaliert)\n",
        "    comm   = V @ P - P @ V\n",
        "    L_comm = (torch.norm(comm, p='fro') ** 2) / V.numel()\n",
        "\n",
        "    # Konsistenz: Modell-Q vs Target-Q\n",
        "    z_S_next_pred, reward_pred = agent.imagine(z_S, actions)\n",
        "    with torch.no_grad():\n",
        "        v_next_pred_target, _ = agent.value_target(z_S_next_pred)\n",
        "    q_pred   = reward_pred + gamma * v_next_pred_target * (1 - dones.unsqueeze(-1))\n",
        "    q_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "    L_cons   = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "    total = L_RL + lambda_kappa * L_rel + lambda_comm * L_comm + lambda_cons * L_cons\n",
        "    return {\n",
        "        'total_loss': total, 'L_RL': L_RL, 'L_rel': L_rel, 'L_comm': L_comm, 'L_cons': L_cons,\n",
        "        'kappa': kappa.mean(), 'td_error': td_err.abs().mean(),\n",
        "        'policy_loss': policy_loss, 'value_loss': value_loss, 'entropy': entropy\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# 4) Einfache 2D-Nav-Umwelt (Reward-Shaping optional)\n",
        "# ==========================================\n",
        "class SimpleControlEnv:\n",
        "    \"\"\"\n",
        "    State: [x, y, goal_x, goal_y, vx, vy] (6D)\n",
        "    Actions: 0:up, 1:down, 2:left, 3:right\n",
        "    Reward:  -distance + progress_bonus * (prev_dist - dist)\n",
        "    Done:    dist < 0.1 oder steps >= max_steps\n",
        "    \"\"\"\n",
        "    def __init__(self, max_steps: int = 200, progress_bonus: float = 0.0):\n",
        "        self.state_dim  = 6\n",
        "        self.action_dim = 4\n",
        "        self.max_steps  = max_steps\n",
        "        self.progress_bonus = progress_bonus\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        self.pos   = np.random.uniform(-1, 1, 2)\n",
        "        self.vel   = np.zeros(2)\n",
        "        self.goal  = np.random.uniform(-1, 1, 2)\n",
        "        self.steps = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        return np.concatenate([self.pos, self.goal, self.vel])\n",
        "\n",
        "    def step(self, action: int):\n",
        "        prev_pos  = self.pos.copy()\n",
        "        prev_dist = float(np.linalg.norm(prev_pos - self.goal))\n",
        "\n",
        "        acc_map = {0: np.array([0, 0.1]), 1: np.array([0, -0.1]),\n",
        "                   2: np.array([-0.1, 0]), 3: np.array([0.1, 0])}\n",
        "        acc  = acc_map[action]\n",
        "        self.vel = np.clip(0.9 * self.vel + acc, -0.5, 0.5)\n",
        "        self.pos = np.clip(self.pos + self.vel, -1,  1)\n",
        "\n",
        "        dist   = float(np.linalg.norm(self.pos - self.goal))\n",
        "        reward = -dist + self.progress_bonus * (prev_dist - dist)\n",
        "\n",
        "        self.steps += 1\n",
        "        done = (dist < 0.1) or (self.steps >= self.max_steps)\n",
        "        return self._get_obs(), reward, done, {'distance': dist}\n",
        "\n",
        "# ==========================================\n",
        "# 5) Replay Buffer\n",
        "# ==========================================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    def push(self, obs, action, reward, next_obs, done):\n",
        "        self.buffer.append((obs, action, reward, next_obs, done))\n",
        "    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
        "        return {\n",
        "            'obs':      torch.FloatTensor(np.array(obs)),\n",
        "            'actions':  torch.LongTensor(actions),\n",
        "            'rewards':  torch.FloatTensor(rewards),\n",
        "            'next_obs': torch.FloatTensor(np.array(next_obs)),\n",
        "            'dones':    torch.FloatTensor(dones)\n",
        "        }\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ==========================================\n",
        "# 6) Training (Warm-up & Ramp & Policy-Mix)\n",
        "# ==========================================\n",
        "def train_stc_alberta(\n",
        "    num_episodes: int = 120,\n",
        "    batch_size: int = 64,\n",
        "    buffer_size: int = 10000,\n",
        "    learning_rate: float = 3e-4,\n",
        "    epsilon_start: float = 0.5,\n",
        "    epsilon_end: float = 0.05,\n",
        "    epsilon_decay: float = 0.992,\n",
        "    print_every: int = 10,\n",
        "    # Zielgewichte (Ende der Ramp)\n",
        "    lambda_kappa: float = 0.05,\n",
        "    lambda_comm: float = 0.02,\n",
        "    lambda_cons: float = 1.0,\n",
        "    gamma: float = 0.99,\n",
        "    entropy_coef: float = 3e-4,\n",
        "    progress_bonus: float = 0.05,\n",
        "    # Scheduling & Policy-Mix\n",
        "    warmup_episodes: int = 20,     # nur RL, kein κ/comm-Druck\n",
        "    ramp_episodes: int = 40,       # danach linear auf Ziel-λ/α\n",
        "    policy_mode: str = \"mix\",      # \"mix\", \"concat\" oder \"zs\"\n",
        "):\n",
        "    env      = SimpleControlEnv(progress_bonus=progress_bonus)\n",
        "    agent    = STCAlbertaAgent(obs_dim=env.state_dim, action_dim=env.action_dim, latent_dim=64, intent_rank=16)\n",
        "    optimizer= torch.optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "    buffer   = ReplayBuffer(capacity=buffer_size)\n",
        "\n",
        "    epsilon  = epsilon_start\n",
        "    ep_rewards, ep_kappas, ep_dists = [], [], []\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"STC–Alberta Agent Training\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Environment: SimpleControlEnv (2D)\")\n",
        "    print(f\"State dim: {env.state_dim}, Action dim: {env.action_dim}\")\n",
        "    print(\"Agent latent dim: 64, Intent rank: 16\")\n",
        "    print(\"=\"*70); print()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # --- Schedule ---\n",
        "        if episode < warmup_episodes:\n",
        "            ramp = 0.0\n",
        "        else:\n",
        "            ramp = min(1.0, (episode - warmup_episodes) / max(1, ramp_episodes))\n",
        "        lambda_kappa_eff = lambda_kappa * ramp\n",
        "        lambda_comm_eff  = lambda_comm  * ramp\n",
        "        mix_alpha        = ramp  # Policy sieht anfangs z, später z_S\n",
        "\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        ep_k_list: List[float] = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon, mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = agent(torch.FloatTensor(obs).unsqueeze(0), mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "                ep_k_list.append(out['kappa'].item())\n",
        "\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "        ep_kappas.append(float(np.mean(ep_k_list)))\n",
        "        ep_dists.append(info['distance'])\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = buffer.sample(batch_size)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(\n",
        "                agent, **batch,\n",
        "                gamma=gamma,\n",
        "                lambda_kappa=lambda_kappa_eff,\n",
        "                lambda_comm=lambda_comm_eff,\n",
        "                lambda_cons=lambda_cons,\n",
        "                entropy_coef=entropy_coef,\n",
        "                mix_alpha=mix_alpha,\n",
        "                policy_mode=policy_mode,\n",
        "            )\n",
        "            losses['total_loss'].backward()\n",
        "            torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "        if (episode + 1) % print_every == 0:\n",
        "            avgR = float(np.mean(ep_rewards[-print_every:]))\n",
        "            avgK = float(np.mean(ep_kappas[-print_every:]))\n",
        "            avgD = float(np.mean(ep_dists[-print_every:]))\n",
        "            msg  = (f\"Episode {episode+1:4d} | Reward: {avgR:7.2f} | κ: {avgK:.3f} \"\n",
        "                    f\"| Dist: {avgD:.3f} | ε: {epsilon:.3f} | ramp: {ramp:.2f} | α: {mix_alpha:.2f}\")\n",
        "            if len(buffer) >= batch_size:\n",
        "                msg += (f\"\\n             | L_RL: {losses['L_RL'].item():.4f} \"\n",
        "                        f\"| L_rel: {losses['L_rel'].item():.4f} \"\n",
        "                        f\"| L_comm: {losses['L_comm'].item():.6f} \"\n",
        "                        f\"| L_cons: {losses['L_cons'].item():.4f} \"\n",
        "                        f\"| H: {losses['entropy'].item():.3f}\")\n",
        "            print(msg)\n",
        "\n",
        "    # Abschluss-Report\n",
        "    k_tail = np.array(ep_kappas[-30:], dtype=np.float32)\n",
        "    r_tail = np.array(ep_rewards[-30:], dtype=np.float32)\n",
        "    corr = float(np.corrcoef(k_tail, r_tail)[0, 1]) if len(k_tail) > 1 else float(\"nan\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*70)\n",
        "    print(\"Training Complete\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Final κ (last 30): {np.mean(k_tail):.3f}\")\n",
        "    print(f\"Final reward (last 30): {np.mean(r_tail):.2f}\")\n",
        "    print(f\"Final distance (last 30): {np.mean(ep_dists[-30:]):.3f}\")\n",
        "    print(f\"Corr(Return, κ) (last 30): {corr:.3f}\")\n",
        "\n",
        "    return agent, ep_rewards, ep_kappas, ep_dists\n",
        "\n",
        "# ==========================================\n",
        "# 7) Ablationen\n",
        "# ==========================================\n",
        "def run_ablation_no_projector(num_episodes: int = 80) -> float:\n",
        "    \"\"\"A1: Kein Π_S (volle latente Fläche).\"\"\"\n",
        "    print(\"\\n\"+\"=\"*70); print(\"ABLATION A1: No Projector (Full Latent Space)\"); print(\"=\"*70)\n",
        "\n",
        "    class NoProjAgent(STCAlbertaAgent):\n",
        "        def forward(self, obs, mix_alpha: float = 1.0, policy_mode: str = \"mix\"):\n",
        "            z   = self.encoder(obs)\n",
        "            z_S = z\n",
        "            P   = torch.eye(self.latent_dim, device=z.device)\n",
        "            k   = torch.ones(z.shape[0], 1, device=z.device)\n",
        "            v, V = self.value_op(z_S)\n",
        "            if policy_mode == \"concat\":\n",
        "                logits = self.policy_concat(torch.cat([z, z_S], dim=-1))\n",
        "            elif policy_mode == \"mix\":\n",
        "                logits = self.policy(z)  # kein Bottleneck\n",
        "            else:\n",
        "                logits = self.policy(z)\n",
        "            return {'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "                    'value_matrix': V, 'kappa': k, 'value': v, 'logits': logits}\n",
        "\n",
        "    env = SimpleControlEnv()\n",
        "    agent = NoProjAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=3e-4)\n",
        "    buffer = ReplayBuffer()\n",
        "    ep_rewards: List[float] = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon=0.1, mix_alpha=0.0, policy_mode=\"mix\")\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= 64:\n",
        "            batch = buffer.sample(64)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch, lambda_kappa=0.0, mix_alpha=0.0, policy_mode=\"mix\")\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    val = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"Final reward (A1, last 30): {val:.2f}\")\n",
        "    return val\n",
        "\n",
        "def run_ablation_random_projector(num_episodes: int = 80) -> float:\n",
        "    \"\"\"A2: Zufälliger (fixierter) Π_S (ohne Lernen) — sollte schlechter sein als gelernter Π_S.\"\"\"\n",
        "    print(\"\\n\"+\"=\"*70); print(\"ABLATION A2: Random Fixed Projector\"); print(\"=\"*70)\n",
        "\n",
        "    class RandProjAgent(STCAlbertaAgent):\n",
        "        def __init__(self, obs_dim, action_dim, latent_dim=64, intent_rank=16):\n",
        "            super().__init__(obs_dim, action_dim, latent_dim, intent_rank)\n",
        "            with torch.no_grad():\n",
        "                Q, _ = torch.linalg.qr(torch.randn(latent_dim, intent_rank))\n",
        "                self.register_buffer('Q_fixed', Q)\n",
        "        def forward(self, obs, mix_alpha: float = 1.0, policy_mode: str = \"mix\"):\n",
        "            z = self.encoder(obs)\n",
        "            Q = self.Q_fixed\n",
        "            P = Q @ Q.T\n",
        "            z_S = z @ P\n",
        "            k   = coherence(z, z_S)\n",
        "            v, V = self.value_op(z_S)\n",
        "            if policy_mode == \"concat\":\n",
        "                logits = self.policy_concat(torch.cat([z, z_S], dim=-1))\n",
        "            elif policy_mode == \"mix\":\n",
        "                feat = (1.0 - mix_alpha) * z + mix_alpha * z_S\n",
        "                logits = self.policy(feat)\n",
        "            else:\n",
        "                logits = self.policy(z_S)\n",
        "            return {'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "                    'value_matrix': V, 'kappa': k, 'value': v, 'logits': logits}\n",
        "\n",
        "    env = SimpleControlEnv()\n",
        "    agent = RandProjAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=3e-4)\n",
        "    buffer = ReplayBuffer()\n",
        "    ep_rewards: List[float] = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon=0.1, mix_alpha=1.0, policy_mode=\"mix\")\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= 64:\n",
        "            batch = buffer.sample(64)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch, mix_alpha=1.0, policy_mode=\"mix\")\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    val = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"Final reward (A2, last 30): {val:.2f}\")\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "IIiw7aTZAgZ3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Schneller Trainingslauf mit Warm-up & Ramp & Policy-Mix\n",
        "agent, rewards, kappas, dists = train_stc_alberta(\n",
        "    num_episodes=120,\n",
        "    print_every=10,\n",
        "    # Zielgewichte (Ende der Ramp):\n",
        "    lambda_kappa=0.05,\n",
        "    lambda_comm=0.02,\n",
        "    lambda_cons=1.0,\n",
        "    entropy_coef=3e-4,\n",
        "    progress_bonus=0.05,\n",
        "    # Scheduling & Policy-Mix:\n",
        "    warmup_episodes=20,\n",
        "    ramp_episodes=40,\n",
        "    policy_mode=\"mix\",   # probiere auch \"concat\" (mehr Kapazität) oder \"zs\"\n",
        ")\n",
        "\n",
        "# Ablationen (optional)\n",
        "a1 = run_ablation_no_projector(num_episodes=80)\n",
        "a2 = run_ablation_random_projector(num_episodes=80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"STC–Alberta (with Π_S): {np.mean(rewards[-30:]):.2f} (last 30 eps)\")\n",
        "print(f\"Ablation A1 (no Π_S):   {a1:.2f} (last 30 eps)\")\n",
        "print(f\"Ablation A2 (rand Π_S): {a2:.2f} (last 30 eps)\")\n",
        "print(f\"Δ vs A1:                {np.mean(rewards[-30:]) - a1:.2f}\")\n",
        "print(f\"Δ vs A2:                {np.mean(rewards[-30:]) - a2:.2f}\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNR7hhVJBGX7",
        "outputId": "e7e3dadc-8eaf-483b-fdb0-115ea9d25c2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STC–Alberta Agent Training\n",
            "======================================================================\n",
            "Environment: SimpleControlEnv (2D)\n",
            "State dim: 6, Action dim: 4\n",
            "Agent latent dim: 64, Intent rank: 16\n",
            "======================================================================\n",
            "\n",
            "Episode   10 | Reward: -315.93 | κ: 0.269 | Dist: 1.401 | ε: 0.461 | ramp: 0.00 | α: 0.00\n",
            "             | L_RL: 0.3986 | L_rel: -0.3415 | L_comm: 0.000020 | L_cons: 2.5067 | H: 1.384\n",
            "Episode   20 | Reward: -254.51 | κ: 0.417 | Dist: 1.279 | ε: 0.426 | ramp: 0.00 | α: 0.00\n",
            "             | L_RL: 0.3628 | L_rel: -0.5387 | L_comm: 0.000024 | L_cons: 1.3725 | H: 1.382\n",
            "Episode   30 | Reward: -214.07 | κ: 0.599 | Dist: 1.096 | ε: 0.393 | ramp: 0.23 | α: 0.23\n",
            "             | L_RL: 0.7117 | L_rel: -0.6061 | L_comm: 0.000026 | L_cons: 1.2599 | H: 1.380\n",
            "Episode   40 | Reward: -271.91 | κ: 0.573 | Dist: 1.368 | ε: 0.363 | ramp: 0.47 | α: 0.47\n",
            "             | L_RL: 0.0724 | L_rel: -0.6211 | L_comm: 0.000026 | L_cons: 0.8590 | H: 1.376\n",
            "Episode   50 | Reward: -276.06 | κ: 0.643 | Dist: 1.305 | ε: 0.335 | ramp: 0.72 | α: 0.72\n",
            "             | L_RL: 0.0296 | L_rel: -0.6878 | L_comm: 0.000027 | L_cons: 0.5875 | H: 1.368\n",
            "Episode   60 | Reward: -276.35 | κ: 0.714 | Dist: 1.227 | ε: 0.309 | ramp: 0.97 | α: 0.97\n",
            "             | L_RL: -0.0581 | L_rel: -0.7469 | L_comm: 0.000028 | L_cons: 0.2720 | H: 1.354\n",
            "Episode   70 | Reward: -289.87 | κ: 0.757 | Dist: 1.539 | ε: 0.285 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.0075 | L_rel: -0.7593 | L_comm: 0.000028 | L_cons: 0.3469 | H: 1.358\n",
            "Episode   80 | Reward: -224.42 | κ: 0.706 | Dist: 1.218 | ε: 0.263 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.1225 | L_rel: -0.7380 | L_comm: 0.000029 | L_cons: 0.2289 | H: 1.364\n",
            "Episode   90 | Reward: -179.68 | κ: 0.695 | Dist: 0.884 | ε: 0.243 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.0469 | L_rel: -0.7173 | L_comm: 0.000029 | L_cons: 0.2339 | H: 1.365\n",
            "Episode  100 | Reward: -155.19 | κ: 0.660 | Dist: 0.589 | ε: 0.224 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.1983 | L_rel: -0.7178 | L_comm: 0.000029 | L_cons: 0.2524 | H: 1.365\n",
            "Episode  110 | Reward: -160.85 | κ: 0.648 | Dist: 0.721 | ε: 0.207 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.0837 | L_rel: -0.7073 | L_comm: 0.000029 | L_cons: 0.2378 | H: 1.369\n",
            "Episode  120 | Reward: -171.43 | κ: 0.654 | Dist: 0.621 | ε: 0.191 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: -0.0257 | L_rel: -0.6912 | L_comm: 0.000030 | L_cons: 0.2963 | H: 1.371\n",
            "\n",
            "======================================================================\n",
            "Training Complete\n",
            "======================================================================\n",
            "Final κ (last 30): 0.654\n",
            "Final reward (last 30): -162.49\n",
            "Final distance (last 30): 0.644\n",
            "Corr(Return, κ) (last 30): -0.184\n",
            "\n",
            "======================================================================\n",
            "ABLATION A1: No Projector (Full Latent Space)\n",
            "======================================================================\n",
            "Final reward (A1, last 30): -194.41\n",
            "\n",
            "======================================================================\n",
            "ABLATION A2: Random Fixed Projector\n",
            "======================================================================\n",
            "Final reward (A2, last 30): -263.04\n",
            "\n",
            "======================================================================\n",
            "RESULTS SUMMARY\n",
            "======================================================================\n",
            "STC–Alberta (with Π_S): -162.49 (last 30 eps)\n",
            "Ablation A1 (no Π_S):   -194.41 (last 30 eps)\n",
            "Ablation A2 (rand Π_S): -263.04 (last 30 eps)\n",
            "Δ vs A1:                31.92\n",
            "Δ vs A2:                100.55\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent, rewards, kappas, dists = train_stc_alberta(\n",
        "    num_episodes=150,\n",
        "    print_every=10,\n",
        "    # Zielgewichte (Ende der Ramp):\n",
        "    lambda_kappa=0.03,\n",
        "    lambda_comm=0.015,\n",
        "    lambda_cons=1.25,\n",
        "    entropy_coef=2e-4,\n",
        "    progress_bonus=0.02,\n",
        "    # Scheduling & Policy-Mix:\n",
        "    warmup_episodes=25,\n",
        "    ramp_episodes=60,\n",
        "    policy_mode=\"concat\",   # auch \"mix\" nochmal testen\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsrFz7SPCAm6",
        "outputId": "a71b671f-e3a2-470a-c0ef-b1e29898657a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STC–Alberta Agent Training\n",
            "======================================================================\n",
            "Environment: SimpleControlEnv (2D)\n",
            "State dim: 6, Action dim: 4\n",
            "Agent latent dim: 64, Intent rank: 16\n",
            "======================================================================\n",
            "\n",
            "Episode   10 | Reward: -238.94 | κ: 0.196 | Dist: 1.284 | ε: 0.461 | ramp: 0.00 | α: 0.00\n",
            "             | L_RL: 0.1427 | L_rel: -0.2557 | L_comm: 0.000021 | L_cons: 1.6618 | H: 1.384\n",
            "Episode   20 | Reward: -280.18 | κ: 0.342 | Dist: 1.395 | ε: 0.426 | ramp: 0.00 | α: 0.00\n",
            "             | L_RL: 0.2597 | L_rel: -0.4674 | L_comm: 0.000024 | L_cons: 1.0221 | H: 1.378\n",
            "Episode   30 | Reward: -221.35 | κ: 0.531 | Dist: 1.198 | ε: 0.393 | ramp: 0.07 | α: 0.07\n",
            "             | L_RL: 0.8424 | L_rel: -0.5785 | L_comm: 0.000025 | L_cons: 0.6470 | H: 1.373\n",
            "Episode   40 | Reward: -311.88 | κ: 0.617 | Dist: 1.652 | ε: 0.363 | ramp: 0.23 | α: 0.23\n",
            "             | L_RL: 0.0903 | L_rel: -0.6158 | L_comm: 0.000024 | L_cons: 0.2395 | H: 1.365\n",
            "Episode   50 | Reward: -217.43 | κ: 0.668 | Dist: 1.178 | ε: 0.335 | ramp: 0.40 | α: 0.40\n",
            "             | L_RL: 0.0913 | L_rel: -0.6869 | L_comm: 0.000024 | L_cons: 0.2518 | H: 1.359\n",
            "Episode   60 | Reward: -244.88 | κ: 0.687 | Dist: 1.335 | ε: 0.309 | ramp: 0.57 | α: 0.57\n",
            "             | L_RL: 0.0726 | L_rel: -0.7012 | L_comm: 0.000025 | L_cons: 0.1765 | H: 1.340\n",
            "Episode   70 | Reward: -248.91 | κ: 0.670 | Dist: 1.150 | ε: 0.285 | ramp: 0.73 | α: 0.73\n",
            "             | L_RL: -0.0058 | L_rel: -0.6464 | L_comm: 0.000025 | L_cons: 0.1529 | H: 1.338\n",
            "Episode   80 | Reward: -263.14 | κ: 0.650 | Dist: 1.336 | ε: 0.263 | ramp: 0.90 | α: 0.90\n",
            "             | L_RL: 0.0140 | L_rel: -0.6423 | L_comm: 0.000026 | L_cons: 0.1289 | H: 1.348\n",
            "Episode   90 | Reward: -143.95 | κ: 0.625 | Dist: 0.677 | ε: 0.243 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.0385 | L_rel: -0.6478 | L_comm: 0.000027 | L_cons: 0.1220 | H: 1.336\n",
            "Episode  100 | Reward: -207.63 | κ: 0.610 | Dist: 0.657 | ε: 0.224 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.1023 | L_rel: -0.5995 | L_comm: 0.000027 | L_cons: 0.1588 | H: 1.328\n",
            "Episode  110 | Reward: -167.78 | κ: 0.544 | Dist: 0.863 | ε: 0.207 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.1666 | L_rel: -0.5721 | L_comm: 0.000027 | L_cons: 0.1499 | H: 1.329\n",
            "Episode  120 | Reward: -218.17 | κ: 0.581 | Dist: 1.097 | ε: 0.191 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.1064 | L_rel: -0.6102 | L_comm: 0.000027 | L_cons: 0.1414 | H: 1.340\n",
            "Episode  130 | Reward: -161.90 | κ: 0.586 | Dist: 0.947 | ε: 0.176 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: -0.0014 | L_rel: -0.6071 | L_comm: 0.000027 | L_cons: 0.1468 | H: 1.362\n",
            "Episode  140 | Reward: -220.14 | κ: 0.603 | Dist: 1.132 | ε: 0.162 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.0108 | L_rel: -0.5852 | L_comm: 0.000028 | L_cons: 0.0655 | H: 1.371\n",
            "Episode  150 | Reward: -179.50 | κ: 0.632 | Dist: 0.883 | ε: 0.150 | ramp: 1.00 | α: 1.00\n",
            "             | L_RL: 0.0045 | L_rel: -0.5948 | L_comm: 0.000028 | L_cons: 0.0711 | H: 1.357\n",
            "\n",
            "======================================================================\n",
            "Training Complete\n",
            "======================================================================\n",
            "Final κ (last 30): 0.607\n",
            "Final reward (last 30): -187.18\n",
            "Final distance (last 30): 0.987\n",
            "Corr(Return, κ) (last 30): -0.762\n"
          ]
        }
      ]
    }
  ]
}