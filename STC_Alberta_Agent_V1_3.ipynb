{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeT8WQv2EkcEX1tyvZi9ox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NNehmer/stc-alberta/blob/main/STC_Alberta_Agent_V1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install --upgrade torch"
      ],
      "metadata": {
        "id": "nBR29Jz70jWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, sys\n",
        "print(\"Torch:\", torch.__version__, \"Python:\", sys.version.split()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_j3V3MK6U7O",
        "outputId": "66fb3aaa-8a1e-454c-d16c-9ecda81e127c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 Python: 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional (Colab): %pip -q install --upgrade torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "# ---------------------------\n",
        "# Reproduzierbarkeit\n",
        "# ---------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# ==========================================\n",
        "# 1) STC-Bausteine\n",
        "# ==========================================\n",
        "class SpectralProjector(nn.Module):\n",
        "    \"\"\"Intentionaler Subraum-Projektor Π_S (QR-orthonormalisiert).\"\"\"\n",
        "    def __init__(self, latent_dim: int, intent_rank: int):\n",
        "        super().__init__()\n",
        "        self.latent_dim  = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "        self.basis = nn.Parameter(torch.randn(latent_dim, intent_rank) * 0.1)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        Q, _ = torch.linalg.qr(self.basis)   # [D, r], orthonormal\n",
        "        Pi_S = Q @ Q.T                       # [D, D]\n",
        "        z_S  = z @ Pi_S\n",
        "        return z_S, Pi_S\n",
        "\n",
        "def coherence(z: torch.Tensor, z_S: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"κ(ψ) = ||Π_S ψ||² / ||ψ||²  (mit keepdim=True).\"\"\"\n",
        "    nz  = torch.norm(z,   dim=-1, keepdim=True) + 1e-8\n",
        "    nzS = torch.norm(z_S, dim=-1, keepdim=True)\n",
        "    return (nzS / nz) ** 2\n",
        "\n",
        "class ValueOperator(nn.Module):\n",
        "    \"\"\"Symmetrischer Wertoperator V; v(ψ)=⟨ψ|V|ψ⟩.\"\"\"\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(latent_dim, latent_dim) * 0.01)\n",
        "\n",
        "    def forward(self, z_S: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        V = 0.5 * (self.W + self.W.T)\n",
        "        v = torch.einsum('bi,ij,bj->b', z_S, V, z_S)\n",
        "        return v.unsqueeze(-1), V\n",
        "\n",
        "# ==========================================\n",
        "# 2) Agent mit Policy-Mix & optionalem Concat-Head\n",
        "# ==========================================\n",
        "class STCAlbertaAgent(nn.Module):\n",
        "    def __init__(self, obs_dim: int, action_dim: int, latent_dim: int = 64, intent_rank: int = 16):\n",
        "        super().__init__()\n",
        "        self.obs_dim     = obs_dim\n",
        "        self.action_dim  = action_dim\n",
        "        self.latent_dim  = latent_dim\n",
        "        self.intent_rank = intent_rank\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128), nn.LayerNorm(128), nn.ReLU(),\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.projector   = SpectralProjector(latent_dim, intent_rank)\n",
        "        self.value_op    = ValueOperator(latent_dim)\n",
        "\n",
        "        # EMA-Target für TD-Stabilität\n",
        "        self.value_target = ValueOperator(latent_dim)\n",
        "        self.value_target.load_state_dict(self.value_op.state_dict())\n",
        "        self.value_tau = 0.005\n",
        "\n",
        "        # Policy \"mix\": gleiche Dim wie z/z_S\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128), nn.ReLU(), nn.Linear(128, action_dim)\n",
        "        )\n",
        "        # Policy \"concat\": optionaler Head auf [z, z_S] (2*latent)\n",
        "        self.policy_concat = nn.Sequential(\n",
        "            nn.Linear(2*latent_dim, 128), nn.ReLU(), nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "        # Dynamik & Reward im z_S-Raum\n",
        "        self.transition = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 128), nn.ReLU(), nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.reward_head = nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 64), nn.ReLU(), nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_value_target(self):\n",
        "        for p, pt in zip(self.value_op.parameters(), self.value_target.parameters()):\n",
        "            pt.data.mul_(1 - self.value_tau).add_(self.value_tau * p.data)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        obs: torch.Tensor,\n",
        "        mix_alpha: float = 1.0,          # 0 → reine z, 1 → reine z_S\n",
        "        policy_mode: str = \"mix\"         # \"mix\", \"concat\" oder \"zs\" (nur z_S)\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        z      = self.encoder(obs)       # [B,D]\n",
        "        z_S, P = self.projector(z)       # [B,D], [D,D]\n",
        "        kappa  = coherence(z, z_S)       # [B,1]\n",
        "        value, V = self.value_op(z_S)    # [B,1], [D,D]\n",
        "\n",
        "        if policy_mode == \"mix\":\n",
        "            feat   = (1.0 - mix_alpha) * z + mix_alpha * z_S\n",
        "            logits = self.policy(feat)\n",
        "        elif policy_mode == \"concat\":\n",
        "            feat   = torch.cat([z, z_S], dim=-1)\n",
        "            logits = self.policy_concat(feat)\n",
        "        elif policy_mode == \"zs\":\n",
        "            logits = self.policy(z_S)\n",
        "        else:\n",
        "            logits = self.policy(z_S)\n",
        "\n",
        "        return {\n",
        "            'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "            'value_matrix': V, 'kappa': kappa, 'value': value, 'logits': logits\n",
        "        }\n",
        "\n",
        "    def imagine(self, z_S: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"actions: [B] (indices) oder [B,A] (one-hot).\"\"\"\n",
        "        if actions.dim() == 1:\n",
        "            a_oh = F.one_hot(actions.long(), num_classes=self.action_dim).float()\n",
        "        elif actions.dim() == 2 and actions.size(-1) == self.action_dim:\n",
        "            a_oh = actions.float()\n",
        "        else:\n",
        "            raise ValueError(\"actions must be [B] (indices) or [B,A] (one-hot)\")\n",
        "        za = torch.cat([z_S, a_oh], dim=-1)\n",
        "        z_S_next = self.transition(za)\n",
        "        reward   = self.reward_head(za)\n",
        "        return z_S_next, reward\n",
        "\n",
        "    def select_action(self, obs: torch.Tensor, epsilon: float = 0.0, mix_alpha: float = 1.0, policy_mode: str = \"mix\") -> int:\n",
        "        with torch.no_grad():\n",
        "            out = self.forward(obs.unsqueeze(0), mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "            if random.random() < epsilon:\n",
        "                return random.randint(0, self.action_dim - 1)\n",
        "            return out['logits'].argmax(dim=-1).item()\n",
        "\n",
        "# ==========================================\n",
        "# 3) Verlustfunktion (inkl. Advantage-Normierung & Policy-Mix)\n",
        "# ==========================================\n",
        "def stc_loss(\n",
        "    agent: STCAlbertaAgent,\n",
        "    obs: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor,\n",
        "    next_obs: torch.Tensor, dones: torch.Tensor,\n",
        "    gamma: float = 0.99,\n",
        "    lambda_kappa: float = 0.1,\n",
        "    lambda_comm: float = 0.01,\n",
        "    lambda_cons: float = 1.0,\n",
        "    entropy_coef: float = 5e-4,\n",
        "    # für die Policy:\n",
        "    mix_alpha: float = 1.0,\n",
        "    policy_mode: str = \"mix\",\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "\n",
        "    out      = agent(obs,      mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "    out_next = agent(next_obs, mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "\n",
        "    z_S, P, V     = out['latent_S'], out['projector'], out['value_matrix']\n",
        "    kappa         = out['kappa']\n",
        "    value, logits = out['value'], out['logits']\n",
        "\n",
        "    dist  = torch.distributions.Categorical(logits=logits)\n",
        "    logp  = dist.log_prob(actions)\n",
        "\n",
        "    # TD-Target mit EMA-Target\n",
        "    with torch.no_grad():\n",
        "        v_next_target, _ = agent.value_target(out_next['latent_S'])\n",
        "        td_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "\n",
        "    td_err = td_target - value\n",
        "\n",
        "    # (a) Advantage-Normierung\n",
        "    adv = td_err.squeeze(-1).detach()\n",
        "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "\n",
        "    policy_loss = -(logp * adv).mean()\n",
        "    value_loss  = F.mse_loss(value, td_target)\n",
        "    entropy     = dist.entropy().mean()\n",
        "    L_RL        = policy_loss + value_loss - entropy_coef * entropy\n",
        "\n",
        "    # Relevanz-Maximierung\n",
        "    L_rel  = -kappa.mean()\n",
        "\n",
        "    # Kommutator-Penalty (Frobenius, skaliert)\n",
        "    comm   = V @ P - P @ V\n",
        "    L_comm = (torch.norm(comm, p='fro') ** 2) / V.numel()\n",
        "\n",
        "    # Konsistenz: Modell-Q vs Target-Q\n",
        "    z_S_next_pred, reward_pred = agent.imagine(z_S, actions)\n",
        "    with torch.no_grad():\n",
        "        v_next_pred_target, _ = agent.value_target(z_S_next_pred)\n",
        "    q_pred   = reward_pred + gamma * v_next_pred_target * (1 - dones.unsqueeze(-1))\n",
        "    q_target = rewards.unsqueeze(-1) + gamma * v_next_target * (1 - dones.unsqueeze(-1))\n",
        "    L_cons   = F.mse_loss(q_pred, q_target)\n",
        "\n",
        "    total = L_RL + lambda_kappa * L_rel + lambda_comm * L_comm + lambda_cons * L_cons\n",
        "    return {\n",
        "        'total_loss': total, 'L_RL': L_RL, 'L_rel': L_rel, 'L_comm': L_comm, 'L_cons': L_cons,\n",
        "        'kappa': kappa.mean(), 'td_error': td_err.abs().mean(),\n",
        "        'policy_loss': policy_loss, 'value_loss': value_loss, 'entropy': entropy\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# 4) Einfache 2D-Nav-Umwelt (Reward-Shaping optional)\n",
        "# ==========================================\n",
        "class SimpleControlEnv:\n",
        "    \"\"\"\n",
        "    State: [x, y, goal_x, goal_y, vx, vy] (6D)\n",
        "    Actions: 0:up, 1:down, 2:left, 3:right\n",
        "    Reward:  -distance + progress_bonus * (prev_dist - dist)\n",
        "    Done:    dist < 0.1 oder steps >= max_steps\n",
        "    \"\"\"\n",
        "    def __init__(self, max_steps: int = 200, progress_bonus: float = 0.0):\n",
        "        self.state_dim  = 6\n",
        "        self.action_dim = 4\n",
        "        self.max_steps  = max_steps\n",
        "        self.progress_bonus = progress_bonus\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        self.pos   = np.random.uniform(-1, 1, 2)\n",
        "        self.vel   = np.zeros(2)\n",
        "        self.goal  = np.random.uniform(-1, 1, 2)\n",
        "        self.steps = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        return np.concatenate([self.pos, self.goal, self.vel])\n",
        "\n",
        "    def step(self, action: int):\n",
        "        prev_pos  = self.pos.copy()\n",
        "        prev_dist = float(np.linalg.norm(prev_pos - self.goal))\n",
        "\n",
        "        acc_map = {0: np.array([0, 0.1]), 1: np.array([0, -0.1]),\n",
        "                   2: np.array([-0.1, 0]), 3: np.array([0.1, 0])}\n",
        "        acc  = acc_map[action]\n",
        "        self.vel = np.clip(0.9 * self.vel + acc, -0.5, 0.5)\n",
        "        self.pos = np.clip(self.pos + self.vel, -1,  1)\n",
        "\n",
        "        dist   = float(np.linalg.norm(self.pos - self.goal))\n",
        "        reward = -dist + self.progress_bonus * (prev_dist - dist)\n",
        "\n",
        "        self.steps += 1\n",
        "        done = (dist < 0.1) or (self.steps >= self.max_steps)\n",
        "        return self._get_obs(), reward, done, {'distance': dist}\n",
        "\n",
        "# ==========================================\n",
        "# 5) Replay Buffer\n",
        "# ==========================================\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    def push(self, obs, action, reward, next_obs, done):\n",
        "        self.buffer.append((obs, action, reward, next_obs, done))\n",
        "    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        obs, actions, rewards, next_obs, dones = zip(*batch)\n",
        "        return {\n",
        "            'obs':      torch.FloatTensor(np.array(obs)),\n",
        "            'actions':  torch.LongTensor(actions),\n",
        "            'rewards':  torch.FloatTensor(rewards),\n",
        "            'next_obs': torch.FloatTensor(np.array(next_obs)),\n",
        "            'dones':    torch.FloatTensor(dones)\n",
        "        }\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ==========================================\n",
        "# 6) Training (Warm-up & Ramp & Policy-Mix + (b) Projector Freeze)\n",
        "# ==========================================\n",
        "def train_stc_alberta(\n",
        "    num_episodes: int = 150,\n",
        "    batch_size: int = 64,\n",
        "    buffer_size: int = 10000,\n",
        "    learning_rate: float = 3e-4,\n",
        "    epsilon_start: float = 0.5,\n",
        "    epsilon_end: float = 0.05,\n",
        "    epsilon_decay: float = 0.992,\n",
        "    print_every: int = 10,\n",
        "    # Zielgewichte (Ende der Ramp)\n",
        "    lambda_kappa: float = 0.02,\n",
        "    lambda_comm: float = 0.01,\n",
        "    lambda_cons: float = 1.3,\n",
        "    gamma: float = 0.99,\n",
        "    entropy_coef: float = 2e-4,\n",
        "    progress_bonus: float = 0.0,\n",
        "    # Scheduling & Policy-Mix\n",
        "    warmup_episodes: int = 25,\n",
        "    ramp_episodes: int = 80,\n",
        "    policy_mode: str = \"concat\",    # \"mix\", \"concat\" oder \"zs\"\n",
        "    alpha_max: float = 0.7,         # maximaler Anteil z_S in der Policy\n",
        "    # Π_S-Rank\n",
        "    intent_rank: int = 8,\n",
        "):\n",
        "    env      = SimpleControlEnv(progress_bonus=progress_bonus)\n",
        "    agent    = STCAlbertaAgent(obs_dim=env.state_dim, action_dim=env.action_dim,\n",
        "                               latent_dim=64, intent_rank=intent_rank)\n",
        "    optimizer= torch.optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "    buffer   = ReplayBuffer(capacity=buffer_size)\n",
        "\n",
        "    epsilon  = epsilon_start\n",
        "    ep_rewards, ep_kappas, ep_dists = [], [], []\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"STC–Alberta Agent Training\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Environment: SimpleControlEnv (2D)\")\n",
        "    print(f\"State dim: {env.state_dim}, Action dim: {env.action_dim}\")\n",
        "    print(f\"Agent latent dim: 64, Intent rank: {intent_rank}\")\n",
        "    print(\"=\"*70); print()\n",
        "\n",
        "    projector_frozen = False\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # --- Schedule ---\n",
        "        if episode < warmup_episodes:\n",
        "            ramp = 0.0\n",
        "        else:\n",
        "            ramp = min(1.0, (episode - warmup_episodes) / max(1, ramp_episodes))\n",
        "\n",
        "        lambda_kappa_eff = lambda_kappa * ramp\n",
        "        lambda_comm_eff  = lambda_comm  * ramp\n",
        "        mix_alpha        = alpha_max * ramp  # α begrenzen\n",
        "\n",
        "        # (b) Projektor nach Ramp-Ende freezen (einmalig)\n",
        "        if (ramp >= 1.0) and (not projector_frozen):\n",
        "            for p in agent.projector.parameters():\n",
        "                p.requires_grad = False\n",
        "            projector_frozen = True\n",
        "            print(\"[info] Projector frozen for late-stage polishing.\")\n",
        "\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        ep_k_list: List[float] = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon,\n",
        "                                         mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "            next_obs, reward, done, info = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = agent(torch.FloatTensor(obs).unsqueeze(0),\n",
        "                            mix_alpha=mix_alpha, policy_mode=policy_mode)\n",
        "                ep_k_list.append(out['kappa'].item())\n",
        "\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "        ep_kappas.append(float(np.mean(ep_k_list)))\n",
        "        ep_dists.append(info['distance'])\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = buffer.sample(batch_size)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(\n",
        "                agent, **batch,\n",
        "                gamma=gamma,\n",
        "                lambda_kappa=lambda_kappa_eff,\n",
        "                lambda_comm=lambda_comm_eff,\n",
        "                lambda_cons=lambda_cons,\n",
        "                entropy_coef=entropy_coef,\n",
        "                mix_alpha=mix_alpha,\n",
        "                policy_mode=policy_mode,\n",
        "            )\n",
        "            losses['total_loss'].backward()\n",
        "            torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "        if (episode + 1) % print_every == 0:\n",
        "            avgR = float(np.mean(ep_rewards[-print_every:]))\n",
        "            avgK = float(np.mean(ep_kappas[-print_every:]))\n",
        "            avgD = float(np.mean(ep_dists[-print_every:]))\n",
        "            msg  = (f\"Episode {episode+1:4d} | Reward: {avgR:7.2f} | κ: {avgK:.3f} \"\n",
        "                    f\"| Dist: {avgD:.3f} | ε: {epsilon:.3f} | ramp: {ramp:.2f} | α: {mix_alpha:.2f}\")\n",
        "            if len(buffer) >= batch_size:\n",
        "                msg += (f\"\\n             | L_RL: {losses['L_RL'].item():.4f} \"\n",
        "                        f\"| L_rel: {losses['L_rel'].item():.4f} \"\n",
        "                        f\"| L_comm: {losses['L_comm'].item():.6f} \"\n",
        "                        f\"| L_cons: {losses['L_cons'].item():.4f} \"\n",
        "                        f\"| H: {losses['entropy'].item():.3f}\")\n",
        "            print(msg)\n",
        "\n",
        "    # Abschluss-Report\n",
        "    k_tail = np.array(ep_kappas[-30:], dtype=np.float32)\n",
        "    r_tail = np.array(ep_rewards[-30:], dtype=np.float32)\n",
        "    corr = float(np.corrcoef(k_tail, r_tail)[0, 1]) if len(k_tail) > 1 else float(\"nan\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*70)\n",
        "    print(\"Training Complete\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Final κ (last 30): {np.mean(k_tail):.3f}\")\n",
        "    print(f\"Final reward (last 30): {np.mean(r_tail):.2f}\")\n",
        "    print(f\"Final distance (last 30): {np.mean(ep_dists[-30:]):.3f}\")\n",
        "    print(f\"Corr(Return, κ) (last 30): {corr:.3f}\")\n",
        "\n",
        "    return agent, ep_rewards, ep_kappas, ep_dists\n",
        "\n",
        "# ==========================================\n",
        "# 7) Ablationen\n",
        "# ==========================================\n",
        "def run_ablation_no_projector(num_episodes: int = 80) -> float:\n",
        "    \"\"\"A1: Kein Π_S (volle latente Fläche).\"\"\"\n",
        "    print(\"\\n\"+\"=\"*70); print(\"ABLATION A1: No Projector (Full Latent Space)\"); print(\"=\"*70)\n",
        "\n",
        "    class NoProjAgent(STCAlbertaAgent):\n",
        "        def forward(self, obs, mix_alpha: float = 0.0, policy_mode: str = \"mix\"):\n",
        "            z   = self.encoder(obs)\n",
        "            z_S = z\n",
        "            P   = torch.eye(self.latent_dim, device=z.device)\n",
        "            k   = torch.ones(z.shape[0], 1, device=z.device)\n",
        "            v, V = self.value_op(z_S)\n",
        "            if policy_mode == \"concat\":\n",
        "                logits = self.policy_concat(torch.cat([z, z_S], dim=-1))\n",
        "            elif policy_mode == \"mix\":\n",
        "                logits = self.policy(z)  # kein Bottleneck\n",
        "            else:\n",
        "                logits = self.policy(z)\n",
        "            return {'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "                    'value_matrix': V, 'kappa': k, 'value': v, 'logits': logits}\n",
        "\n",
        "    env = SimpleControlEnv()\n",
        "    agent = NoProjAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=3e-4)\n",
        "    buffer = ReplayBuffer()\n",
        "    ep_rewards: List[float] = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon=0.1, mix_alpha=0.0, policy_mode=\"mix\")\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= 64:\n",
        "            batch = buffer.sample(64)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch, lambda_kappa=0.0, mix_alpha=0.0, policy_mode=\"mix\")\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    val = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"Final reward (A1, last 30): {val:.2f}\")\n",
        "    return val\n",
        "\n",
        "def run_ablation_random_projector(num_episodes: int = 80) -> float:\n",
        "    \"\"\"A2: Zufälliger (fixierter) Π_S (ohne Lernen) — sollte schlechter sein als gelernter Π_S.\"\"\"\n",
        "    print(\"\\n\"+\"=\"*70); print(\"ABLATION A2: Random Fixed Projector\"); print(\"=\"*70)\n",
        "\n",
        "    class RandProjAgent(STCAlbertaAgent):\n",
        "        def __init__(self, obs_dim, action_dim, latent_dim=64, intent_rank=16):\n",
        "            super().__init__(obs_dim, action_dim, latent_dim, intent_rank)\n",
        "            with torch.no_grad():\n",
        "                Q, _ = torch.linalg.qr(torch.randn(latent_dim, intent_rank))\n",
        "                self.register_buffer('Q_fixed', Q)\n",
        "        def forward(self, obs, mix_alpha: float = 1.0, policy_mode: str = \"mix\"):\n",
        "            z = self.encoder(obs)\n",
        "            Q = self.Q_fixed\n",
        "            P = Q @ Q.T\n",
        "            z_S = z @ P\n",
        "            k   = coherence(z, z_S)\n",
        "            v, V = self.value_op(z_S)\n",
        "            if policy_mode == \"concat\":\n",
        "                logits = self.policy_concat(torch.cat([z, z_S], dim=-1))\n",
        "            elif policy_mode == \"mix\":\n",
        "                feat = (1.0 - mix_alpha) * z + mix_alpha * z_S\n",
        "                logits = self.policy(feat)\n",
        "            else:\n",
        "                logits = self.policy(z_S)\n",
        "            return {'latent': z, 'latent_S': z_S, 'projector': P,\n",
        "                    'value_matrix': V, 'kappa': k, 'value': v, 'logits': logits}\n",
        "\n",
        "    env = SimpleControlEnv()\n",
        "    agent = RandProjAgent(obs_dim=6, action_dim=4, latent_dim=64, intent_rank=16)\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=3e-4)\n",
        "    buffer = ReplayBuffer()\n",
        "    ep_rewards: List[float] = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(torch.FloatTensor(obs), epsilon=0.1, mix_alpha=1.0, policy_mode=\"mix\")\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "            buffer.push(obs, action, reward, next_obs, float(done))\n",
        "            obs = next_obs\n",
        "            ep_reward += reward\n",
        "\n",
        "        ep_rewards.append(ep_reward)\n",
        "\n",
        "        if len(buffer) >= 64:\n",
        "            batch = buffer.sample(64)\n",
        "            optimizer.zero_grad()\n",
        "            losses = stc_loss(agent, **batch, mix_alpha=1.0, policy_mode=\"mix\")\n",
        "            losses['total_loss'].backward()\n",
        "            optimizer.step()\n",
        "            agent.update_value_target()\n",
        "\n",
        "    val = float(np.mean(ep_rewards[-30:]))\n",
        "    print(f\"Final reward (A2, last 30): {val:.2f}\")\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "5iKW07WSEKq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent, rewards, kappas, dists = train_stc_alberta(\n",
        "    num_episodes=150,\n",
        "    print_every=10,\n",
        "    # Zielgewichte (Ende der Ramp):\n",
        "    lambda_kappa=0.02,\n",
        "    lambda_comm=0.01,\n",
        "    lambda_cons=1.3,\n",
        "    entropy_coef=2e-4,\n",
        "    progress_bonus=0.0,     # erst ohne Shaping messen\n",
        "    # Scheduling & Policy-Mix:\n",
        "    warmup_episodes=25,\n",
        "    ramp_episodes=80,\n",
        "    policy_mode=\"concat\",\n",
        "    alpha_max=0.7,\n",
        "    # Π_S-Rank:\n",
        "    intent_rank=8,\n",
        ")\n",
        "\n",
        "a1 = run_ablation_no_projector(num_episodes=80)\n",
        "a2 = run_ablation_random_projector(num_episodes=80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"STC–Alberta (with Π_S): {np.mean(rewards[-30:]):.2f} (last 30 eps)\")\n",
        "print(f\"Ablation A1 (no Π_S):   {a1:.2f} (last 30 eps)\")\n",
        "print(f\"Ablation A2 (rand Π_S): {a2:.2f} (last 30 eps)\")\n",
        "print(f\"Δ vs A1:                {np.mean(rewards[-30:]) - a1:.2f}\")\n",
        "print(f\"Δ vs A2:                {np.mean(rewards[-30:]) - a2:.2f}\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR5wvs2bEe7m",
        "outputId": "33848564-ef0b-465d-e8e6-da2b5d30994b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STC–Alberta Agent Training\n",
            "======================================================================\n",
            "Environment: SimpleControlEnv (2D)\n",
            "State dim: 6, Action dim: 4\n",
            "Agent latent dim: 64, Intent rank: 8\n",
            "======================================================================\n",
            "\n",
            "Episode   10 | Reward: -258.67 | κ: 0.130 | Dist: 1.215 | ε: 0.461 | ramp: 0.00 | α: 0.00\n",
            "             | L_RL: 1.8712 | L_rel: -0.2220 | L_comm: 0.000011 | L_cons: 2.2977 | H: 1.381\n",
            "Episode   20 | Reward: -280.11 | κ: 0.356 | Dist: 1.572 | ε: 0.426 | ramp: 0.00 | α: 0.00\n",
            "             | L_RL: 0.5021 | L_rel: -0.4696 | L_comm: 0.000016 | L_cons: 2.0876 | H: 1.371\n",
            "Episode   30 | Reward: -246.45 | κ: 0.518 | Dist: 1.247 | ε: 0.393 | ramp: 0.05 | α: 0.03\n",
            "             | L_RL: 0.3288 | L_rel: -0.5086 | L_comm: 0.000017 | L_cons: 1.7187 | H: 1.363\n",
            "Episode   40 | Reward: -269.08 | κ: 0.508 | Dist: 1.375 | ε: 0.363 | ramp: 0.17 | α: 0.12\n",
            "             | L_RL: 0.2825 | L_rel: -0.5323 | L_comm: 0.000016 | L_cons: 1.1106 | H: 1.351\n",
            "Episode   50 | Reward: -285.64 | κ: 0.572 | Dist: 1.428 | ε: 0.335 | ramp: 0.30 | α: 0.21\n",
            "             | L_RL: 0.2367 | L_rel: -0.6161 | L_comm: 0.000017 | L_cons: 0.6307 | H: 1.310\n",
            "Episode   60 | Reward: -264.21 | κ: 0.630 | Dist: 1.217 | ε: 0.309 | ramp: 0.42 | α: 0.30\n",
            "             | L_RL: 0.0292 | L_rel: -0.6706 | L_comm: 0.000017 | L_cons: 0.3445 | H: 1.205\n",
            "Episode   70 | Reward: -258.35 | κ: 0.672 | Dist: 1.280 | ε: 0.285 | ramp: 0.55 | α: 0.39\n",
            "             | L_RL: 0.3896 | L_rel: -0.7063 | L_comm: 0.000017 | L_cons: 0.2234 | H: 1.044\n",
            "Episode   80 | Reward: -329.25 | κ: 0.720 | Dist: 1.598 | ε: 0.263 | ramp: 0.68 | α: 0.47\n",
            "             | L_RL: 0.1042 | L_rel: -0.6948 | L_comm: 0.000018 | L_cons: 0.1369 | H: 0.991\n",
            "Episode   90 | Reward: -290.66 | κ: 0.672 | Dist: 1.563 | ε: 0.243 | ramp: 0.80 | α: 0.56\n",
            "             | L_RL: 0.0046 | L_rel: -0.6815 | L_comm: 0.000018 | L_cons: 0.3139 | H: 0.955\n",
            "Episode  100 | Reward: -202.02 | κ: 0.642 | Dist: 1.042 | ε: 0.224 | ramp: 0.93 | α: 0.65\n",
            "             | L_RL: 0.1519 | L_rel: -0.6636 | L_comm: 0.000018 | L_cons: 0.2852 | H: 0.767\n",
            "[info] Projector frozen for late-stage polishing.\n",
            "Episode  110 | Reward: -256.15 | κ: 0.631 | Dist: 1.357 | ε: 0.207 | ramp: 1.00 | α: 0.70\n",
            "             | L_RL: -0.1518 | L_rel: -0.6278 | L_comm: 0.000019 | L_cons: 0.2374 | H: 0.618\n",
            "Episode  120 | Reward: -211.12 | κ: 0.562 | Dist: 0.901 | ε: 0.191 | ramp: 1.00 | α: 0.70\n",
            "             | L_RL: -0.0607 | L_rel: -0.6079 | L_comm: 0.000019 | L_cons: 0.1606 | H: 0.563\n",
            "Episode  130 | Reward: -262.58 | κ: 0.584 | Dist: 1.539 | ε: 0.176 | ramp: 1.00 | α: 0.70\n",
            "             | L_RL: 0.1294 | L_rel: -0.6080 | L_comm: 0.000020 | L_cons: 0.2414 | H: 0.542\n",
            "Episode  140 | Reward: -268.41 | κ: 0.586 | Dist: 1.294 | ε: 0.162 | ramp: 1.00 | α: 0.70\n",
            "             | L_RL: 0.0317 | L_rel: -0.6014 | L_comm: 0.000021 | L_cons: 0.1386 | H: 0.575\n",
            "Episode  150 | Reward: -291.33 | κ: 0.660 | Dist: 1.470 | ε: 0.150 | ramp: 1.00 | α: 0.70\n",
            "             | L_RL: -0.1556 | L_rel: -0.6344 | L_comm: 0.000021 | L_cons: 0.1578 | H: 0.602\n",
            "\n",
            "======================================================================\n",
            "Training Complete\n",
            "======================================================================\n",
            "Final κ (last 30): 0.610\n",
            "Final reward (last 30): -274.11\n",
            "Final distance (last 30): 1.434\n",
            "Corr(Return, κ) (last 30): -0.818\n",
            "\n",
            "======================================================================\n",
            "ABLATION A1: No Projector (Full Latent Space)\n",
            "======================================================================\n",
            "Final reward (A1, last 30): -270.58\n",
            "\n",
            "======================================================================\n",
            "ABLATION A2: Random Fixed Projector\n",
            "======================================================================\n",
            "Final reward (A2, last 30): -235.97\n",
            "\n",
            "======================================================================\n",
            "RESULTS SUMMARY\n",
            "======================================================================\n",
            "STC–Alberta (with Π_S): -274.11 (last 30 eps)\n",
            "Ablation A1 (no Π_S):   -270.58 (last 30 eps)\n",
            "Ablation A2 (rand Π_S): -235.97 (last 30 eps)\n",
            "Δ vs A1:                -3.53\n",
            "Δ vs A2:                -38.14\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}